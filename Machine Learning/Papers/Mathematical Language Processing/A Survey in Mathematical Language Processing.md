---
title: "A Survey in Mathematical Language Processing "
authors:
    - Jordan Meadows
    - André Freitas
url:
    - "[paper](https://arxiv.org/pdf/2205.15231.pdf)"
    - "[arxiv](https://arxiv.org/abs/2205.15231)"
published: "2022-05-30"
---

# A Survey in Mathematical Language Processing 
###### Authors
<ul>
<li class="author">Jordan Meadows</li>
<li class="separator author">|</li>
<li class="author">André Freitas</li>
</ul>

```dataview
table without ID url from "Mathematics/Machine Learning/Papers/Mathematical Language Processing/A Survey in Mathematical Language Processing.md"
```

## TL;DR

The text presents key concepts in the evolution of mathematical understanding and reasoning in artificial intelligence. 

1. **Identifier-definition extraction**: Progression from rule-based methods to machine learning approaches, with improved text vector representations, notably pre-trained embeddings like BERT.
   
2. **Math Information Retrieval (MIR)**: Improvement in formula retrieval systems, using symbol layout trees and operator trees, although challenges such as wildcard queries and diverse formulae persist.
   
3. **Premise selection in theorem proving**: The distinction between formal and informal premise selection with the latter dealing with natural language and LaTeX and presenting opportunities for improvement.
   
4. **Math word problem solving**: Advancements in dependency graph utilization and multi-encoder/multi-decoder architectures, but limitations in handling complex relations and integrating external knowledge still exist.
   
5. **Autoformalisation in theorem proving**: The difficulties in translating informal mathematical text into formal language and the exploration of various strategies to address this, such as direct translation and approximate translation through formula embeddings.
   
6. **Development of robust, interactive natural language theorem provers**: The importance and challenges of creating these provers to make theorem proving more accessible and intuitive.


#### Abstract

Informal mathematical text underpins realworld quantitative reasoning and communication. Developing sophisticated methods of retrieval and abstraction from this dual modality is crucial in the pursuit of the vision of automating discovery in quantitative science and mathematics. We track the development of informal mathematical language processing approaches across five strategic sub-areas in recent years, highlighting the prevailing successful methodological elements along with existing limitations.


## 1 Introduction

Communicating quantitative science occurs through the medium of mathematical text, which contains expressions, formulae, and equations, most of which requiring accompanying description. Formulae and their explanations interweave with non-mathematical language to form cohesive discourse. Approaches that consider mathematical text have been proposed to solve a number of related tasks, but are yet to surpass human-level performance. Core areas include:
- solving math word problems,
- identifier-definition extraction and variable typing, 
- natural language premise selection, 
- natural language theorem proving, 
- and formula retrieval. 

While transformers have seen widespread success in many areas of language, it is not until recently they've demonstrated mathematical and logical capabilities, since redefining state-of-the-art benchmarks in formula retrieval and solving math word problems. Alongside transformers, graph neural networks (GNNs) also exhibit diverse reasoning capabilities with respect to mathematical language, including premise selection and mathematical question answering, up to algebraic manipulation. There is a clear evolutionary path in mathematical language processing, from roots in explicit discourse representation to the present day, where graph-based and transformerbased models deliver leading metrics in a few related tasks, complemented by explicit methods in some cases. This survey provides a synthesis of this evolutionary arch: in Section 2 we discuss research contributions leading to the current state-of-the-art for each task where applicable, ending each discussion with notable limitations of the strongest approaches. In Section 3 we conclude, discussing promising directions for future research involving informal mathematics.

## 2 Representative Areas

Research considering the link between mathematics and language has diversified since early work with math word problems (Feigenbaum et al., 1963; Bobrow, 1964; Charniak, 1969) and discourse representation theory-based linguistic analysis of formal theorems (Zinn, 1999, 2003). Contemporary focal areas are driven by target textual interpretation and inference tasks, with a significant emphasis on the empirical evaluation of models. Examples of such areas include identifierdefinition extraction, math information retrieval and formula search, natural language premise selection, math word problem solving, and informal theorem proving. We project these areas into an inference spectrum displayed in Figure 1.

> [!blank-container]
> 
> ![[figure_1.png]]
> > **Figure 1**: Extractive tasks are closer to the lexical and surface-level expression of the text while abstractive tasks tend to require the integration of symbolic-level and abstract reasoning.
> 

### 2.1 Identifier-Definition Extraction


> [!definition] 
> Identifier-definition extraction is a process in Natural Language Processing (NLP) that involves identifying and linking variables or identifiers with their corresponding descriptions or definitions within a given text. This task is crucial in understanding mathematical, scientific, or technical discourses where variables or identifiers are commonly used. These descriptions are typically found in the local context of the first instance of the identifiers in the discourse.


> [!summary] 
> The evolution of identifier-definition extraction and related tasks have shifted from rule-based methods to machine learning approaches, with each task variant relying on distinct datasets. These tasks face significant variability in defining the scope from which identifiers are linked to descriptions, ranging from localized document-level context to external resources. Over time, the vector representations of text in these tasks have evolved from feature-based vectors to pre-trained embeddings such as BERT, enhancing the capabilities of downstream tasks like variable typing and mathematical notation modeling.



A significant proportion of variables or identifiers in formulae or text are explicitly defined within a discourse context (Wolska and Grigore, 2010). Descriptions are usually local to the first instance of the identifiers in the discourse. It is the broad goal of identifier-definition extraction and related tasks to pair-up identifiers with their counterpart descriptions.

The task has not converged to a canonical form. Despite the clarity of its overall aim, the task has materialised into different forms: Kristianto et al. (2012) predict descriptions given expressions, Pagael and Schubotz (2014) predict descriptions given identifiers through identifier-definition extraction, Stathopoulos et al. (2018) predict if a type matches a variable through variable typing, and Jo et al. (2021) predict notation given context through notation auto-suggestion and notation consistency checking tasks. More concretely, identifier-definition extraction (Schubotz et al., 2016a) involves scoring identifier-definiens pairs, where a definiens is a potential natural language description of the identifier. Given graph nodes from predefined variables $V$ and types $T$, variable typing (Stathopoulos et al., 2018) is the task of classifying whether edges $V \times T$ are either existent (positive) or non-existent (negative), where a positive classification means a variable matches with the type. Notation auto-suggestion (Jo et al., 2021) uses the text of both the sentence containing notation and the previous sentence to model future notation from the vocabulary of the tokenizer. The evolution of the overall area can be traced from an early ranking task (Pagael and Schubotz, 2014) reliant on heuristics and rules (Alexeeva et al., 2020), through ML-based edge classification (Stathopoulos et al., 2018), to language modelling with Transformers (Jo et al., 2021). Different datasets are proposed for each task variant.

There is a high variability in scoping definitions. The scope from which identifiers are linked to descriptions varies significantly, and it is difficult to compare model performance even when tackling the same variant of the task (Schubotz et al., 2017; Alexeeva et al., 2020). At a local context, models such as Pagael and Schubotz (2014) and Alexeeva et al. (2020) match identifiers with definitions from the same document "as the author intended", while other identifier-definition extraction methods (Schubotz et al., 2016a, 2017) rely on data external to a given document, such as links to semantic concepts on Wikidata and NTCIR-11 test data (Schubotz et al., 2015). At a broader context, the variable typing model proposed in Stathopoulos et al. (2018) relies on an external dictionary of types (Stathopoulos and Teufel, 2015, 2016; Stathopoulos et al., 2018) extracted from both the Encyclopedia of Mathematics[^1] and Wikipedia.

Vector representations have evolved to transfer knowledge from previous tasks, allowing downstream variable typing tasks to benefit from pretrained natural language embeddings. Overall, vector representations of text have evolved from feature-based vectors learned from scratch for a single purpose, to the modern paradigm of pre-trained embeddings re-purposed for novel tasks. Kristianto et al. (2012) input pattern features into a conditional random fields model for the purpose of identifying definitions of expressions in LaTeX papers. Kristianto et al. (2014a) learn vectors through a linear-kernel SVM with input features comprising of sentence patterns, part-of-speech (POS) tags, and tree structures. Stathopoulos et al. (2018) extend this approach by adding type- and variable-centric features as a baseline also with a linear ker- nel. Alternatively, Schubotz et al. (2017) use a Gaussian scoring function (Schubotz et al., 2016b) and pattern matching features (Pagael and Schubotz, 2014) as input to an SVM with a radial basis function (RBF) kernel, to account for non-linear feature characteristics. Alternative classification approaches (Kristianto et al., 2012; Stathopoulos et al., 2018) do not use input features derived from non-linear functions, such as the Gaussian scoring function, and hence use linear kernels. Embedding spaces have been learned in this context for the purpose of ranking identifier-definiens pairs through latent semantic analysis at the document level, followed by the application of clustering techniques and methods of relating clusters to namespaces inherited from software engineering (Schubotz et al., 2016a). These cluster-based namespaces are later used for classification (Schubotz et al., 2017) rather than ranking, but do not positively impact SVM model performance, despite previous evidence suggesting they resolve co-references (Duval et al., 2002) such as " $E$ is energy" and " $E$ is expectation value". Neither clustering nor namespaces have been further explored in this context. While a more recent model learns context-specific word representations after feeding less specific pre-trained word2vec (Mikolov et al., 2013; Stathopoulos and Teufel, 2016) embeddings to a bidirectional LSTM for classification (Stathopoulos et al., 2018), the most recent work predictably relies on more sophisticated pre-trained BERT embeddings (Devlin et al., 2018) for the language modelling of mathematical notation (Jo et al., 2021).

[^1]: https://encyclopediaofmath.org

| Work | Task | Learning | Approach | Dataset | Metrics | ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=21&width=140&top_left_y=217&top_left_x=1415) | Key Representation |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $\begin{array}{l}\text { Iddentifich-Definition Extrat } \\ \text { Kristanto e a l. } 2012)\end{array}$ | $\begin{array}{l}\text { Expression-definition } \\ \text { Elon }\end{array}$ | s | CRF with linguistic pattern features | LaTeX papers | P. R. F1 | MathML | Definition noun phrases |
| $\begin{array}{l}\text { Kristianto e tal. (2014a) } \\ \text { Pagael ad Schubol } 2014)\end{array}$ | ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=30&width=190&top_left_y=265&top_left_x=412) | $\underset{\mathrm{R}}{\mathrm{S}}$ | $\begin{array}{l}\text { SVM with linguistic pattern features } \\ \text { Gaussinherisic raking }\end{array}$ | $\begin{array}{l}\text { LaTeX papers } \\ \text { Wikipedia a ticles }\end{array}$ | ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=30&width=165&top_left_y=265&top_left_x=1251) | MathML | $\begin{array}{l}\text { Definition noun phrasess } \\ \text { Iddef explicitemplates }\end{array}$ |
| Schubotz et al. (2016a) | ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=29&width=190&top_left_y=289&top_left_x=412) | uNS | ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=31&width=343&top_left_y=287&top_left_x=678) | $\begin{array}{l}\text { Wikpedia aricles } \\ \text { NTCII.11 Math Wipedia }\end{array}$ | $\underset{\mathrm{P}, \mathrm{R}, \mathrm{FI}}{ }$ | LaTeX | $\begin{array}{l}\text { II-def expplict tempplates } \\ \text { Namesae custers }\end{array}$ |
| $\begin{array}{l}\text { Schubote et al. (2017) } \\ \text { Stathopollos } 2018)\end{array}$ | $\begin{array}{l}\text { IIentufirerdefinition } \\ \text { Varable Typing }\end{array}$ | $\begin{array}{l}\mathrm{s} \\ \mathrm{s}\end{array}$ | $\begin{array}{l}\text { G. rank + patter matchhing + SMM } \\ \text { Link predicion wiht BiLTM }\end{array}$ | $\begin{array}{l}\text { NTCCR-11 Math Wikipedia } \\ \text { arXiv pers }\end{array}$ | $\begin{array}{l}\substack{P, R, F 1 \\ P, R, F 1}\end{array}$ | $\begin{array}{l}\text { LaTeX } \\ \text { MathML }\end{array}$ | $\begin{array}{l}\text { Patarm matching SVM Eeatures } \\ \text { Type dictionar extended wihh DSTA }\end{array}$ |
| Alexeva et al. (2020) | $\begin{array}{l}\text { Identifier-defininition } \\ \text { Id }\end{array}$ | R | Odin grammar | MathAlign-Eval | ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=25&width=165&top_left_y=342&top_left_x=1251) | LaTeX | LaTeX segmentation and alignment |
| Jo et al. (2021) | ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=31&width=190&top_left_y=364&top_left_x=412) | s | BERT fine-tuning | S2ORC | Top1, Top5, MRR | LaTeX | LaTeX macro representations |
| $\begin{array}{l}\text { Formulal Retrieval } \\ \text { Kristanto e and (2014b) }\end{array}$ | NTCIR-11 Math-2 | $S+R$ | SVM description extraction + leaffroot path search | NTCIR-11 Math-2 | Р@5,P@10, MAP | MathML | MathML leaf-root paths |
| Kristianto e tal (2016) | NTCIR-12 MathIR | $\mathrm{s}+\mathrm{R}$ | MCAT (2014) + multiple linear regression | NTCIR-12 MathIR | P@K | MathML | hash-based encoding and dependency graph |
| Zanibbi e tal. (2016b) | $\begin{array}{l}\text { NNCRI-111 Wikipedia } \\ \text { Formula Rerteval }\end{array}$ | $\mathrm{R}$ | Inverted index ranking + MSS reranking search | NTCIR-11 Wikipedia | R@K, MRR | MathML | SLT leaf-root path tuples |
| Davila and Zanibbi (2017) | $\begin{array}{l}\text { NTCIR-12 Wikipedia } \\ \text { Formularowsing }\end{array}$ | s | ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=33&width=343&top_left_y=471&top_left_x=678) | $\begin{array}{l}\text { NTCRR-12 Wikipedia } \\ \text { Formula rewsing }\end{array}$ | P@K, Bpref, nDCG@K | LaTeX + MathML | SLT + OPT leaf-rooot path tuples |
| Zhong and Zanibbi (2019) | $\begin{array}{l}\text { NTCIR-12 Wikipedia } \\ \text { Formun Bowsing }\end{array}$ | R | $\begin{array}{l}\text { OPTT leaf-root path search } \\ \text { with }\end{array}$ | NTCIR-12 Wikipedia | Р@K, Bpref | LaTeX | OPT leat-root path tuples and subexpressions |
| Mansouri et al. (2019) | $\begin{array}{l}\text { NTCRR-12 Wikipedia } \\ \text { Norpwl Bowsind }\end{array}$ | uns | n-gram fastrext oPT and SLT embeddings | NTCIR-12 Wikipedia | Bpref@ 1000 | LaTeX + MathML | SLT and OPT leaf-root path tuple n-grams |
| Peng et al (2021) | NTCIR-12 Wikipedia | ss | pre-training BERT with tasks related | NTCIR-12 Wikipedia | Bpref@ 1000 | LaTeX | ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=30&width=254&top_left_y=563&top_left_x=1559) |
| Informal Premise Selection | Formula browsing + ofiners | ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=31&width=61&top_left_y=592&top_left_x=610) | ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=31&width=343&top_left_y=592&top_left_x=678) | Formula browsung | ت | de | ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=31&width=254&top_left_y=592&top_left_x=1559) |
| Ferreira and Freitas (2020b) | $\begin{array}{c}\text { Natural Language } \\ \text { Premis Selection }\end{array}$ | s | DGCNN for link prediction | PS-Proofwiki | P, R, FI | LaTex | Statement dependency graph |
| Ferreira and Freitas (2021) | Naturalal Language | s | Self-attention for math and language | PS.Proorwiki | P, R. F1 | LaTex | Cross-model encoding for math and NL |
| Coavoux and Cohen (2021) | $\begin{array}{l}\text { Premilese belection } \\ \text { Statcmenterof Mang }\end{array}$ | s | Weighted bipartite matching + self-attention | SPM | $\mathrm{MRR}+\mathrm{Acc}$ | MathML | Self-attention encoding + bilinear similarity |
| Han et al. (2021) | Informal premise selection | s | LLM fine-tuning (webext + webmath) | Naturalifroofs | R@K, avgæ@K, full@K | LaTeX | Transformer encodings |
| ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=28&width=162&top_left_y=714&top_left_x=253) | Mathematical Reference Retrieval | $s$ | Fine-tuning BERT with pairfjoint parameterization | NaturalProofs | MAP, R@K, full@K | LaTeX | BERT encodings |
| $\begin{array}{l}\text { Liu et al. (2019) } \\ \text { Xie nd Sum } 2009)\end{array}$ | Math Word Problem Solving | ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=31&width=61&top_left_y=741&top_left_x=610) | $\begin{array}{l}\text { BiLSTM seq encoder + LSTM tree-based decooder } \\ \text { GRT encodr tGSTdecoder }\end{array}$ | Math23K | Acc | NL | $\begin{array}{l}\text { Abstract Syntax Tree (AST) } \\ \text { Govidive }\end{array}$ |
| Li et al. (2020) | Math Word Problem Solving | s | $\begin{array}{l}\text { (word-word graph + phrase structure graph) } \\ \text { heterogeneous graph encoder + LSTM tree-based decoder }\end{array}$ | MAWPS, MATHQA | Acc | NL | Dependency parse tree + constituency tree |
| Zhang et al. (2020) | Math Word Problem Solving | s | $\begin{array}{l}\text { Word-number graph encoder }+ \\ \text { Numbercome raph encoder } G \text { decoder }\end{array}$ | MAWPS, Math23K | Acc | NL | Word-number graph + Number comp graph |
| Shen and Jin 2020$)$ | Math Word Problem Solving | s | Sequ multi-encoder trree-based multi-decoder | Math23K | Acc | NL | Multiencoding/decoding) |
| Kim et al. (2020) | Math Word Problem Solving | s | ALBERT seq encoder + Transformer seq decoder | ALG514, DRAW-1K, MAWPS | Acc | NL | ALBERT encodings |
| Qin et al. (2020) | Math Word Problem Solving | s | ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=31&width=343&top_left_y=870&top_left_x=678) | HмwP, ALG514, Math23K, Dolphin18K | Acc | NL | Universal Expression Tree (UET) |
| Cao et al. (2021) | Math Word Problem Solving | s | $\begin{array}{l}\text { GRU-based encoder r DAG-LSTM decoder } \\ \text { KR-bed }\end{array}$ | DRAW-1K, Math23K | Acc | $\mathrm{NL}$ | Directed Acyclic Graph (DAG) |
| $\begin{array}{l}\text { Line etal. (2021) } \\ \text { Onine tal } 2021)\end{array}$ | $\begin{array}{l}\text { Math Wor Arobblem Solving } \\ \text { Math Word Probele Solving }\end{array}$ | s | $\begin{array}{l}\text { Hierarchical lRUU seq encodor + +GSS decooder } \\ \text { Bi.GGU encoder G GTS Secoder with atention and UET }\end{array}$ | $\begin{array}{l}\text { Math23K, AAWPS } \\ \text { Mah3K CM17K }\end{array}$ | AAce | ${ }_{\mathrm{NL}}^{\mathrm{NL}}$ | $\begin{array}{l}\text { Hierarchical word-clause-problem encodings } \\ \text { Represtatios from Auxiliar Tasss }\end{array}$ |
| Liang et al. (2021) | Math Word Problem Solving | s | $\begin{array}{l}\text { BERT encoder }+G T S \text { decoder }\end{array}$ | Math23K, APE210K | Acc | $\mathrm{NL}$ | BERT encodings |
| Zhang et al. (2022) | Math Word Problem Solving | s | $\begin{array}{l}\text { (word-word graph + word-Dumber graph + number-comp graph) } \\ \text { heterogneous raph nocoder GTSSdecoder }\end{array}$ | MAWPS, Math23K | Acc | NL | word-word, word-num, num-comp het. gr. enc. |
| $\begin{array}{l}\text { Informal Theorem Proving } \\ \text { KKaliszkk e tal. } 2015 \text { ) }\end{array}$ | Autoformalisation | s | Informal symbol sentence parsing with probabilistic CFGs | HOL Light + Flyspeck |  | LaTeX to HOL/Flyspeck | HOL parse trees |
| Wang et al. (2020) | Autoformalisation | $s+U N S$ | Machine translation wih RNNS, LSTMs and Transformers | LaTeX, Mizar, TPTP, Proofwiki | $\begin{array}{l}\text { BLEU, Perplexity } \\ \text { Editidtance }\end{array}$ | LaTeX, Mizar, TPTP | NMT, UNMT, XLM seq encodings |
| Meadows and Freitas (2021) | ![](https://cdn.mathpix.com/cropped/2023_06_28_d3c1302e5dfcf527f7d0g-03.jpg?height=31&width=190&top_left_y=1059&top_left_x=412) | ${ }_{\mathrm{R}}$ | Similarity-based search with | PhysA-368 - | Acc | LaTex | LaTeX subexpression |
| Welleck et al. (2021a) | $\begin{array}{l}\text { Aulomancic cal Devination } \\ \text { Matheration Rerence Generaion }\end{array}$ | s | singngmiencs suns subexpression hiunstics | Naturalifroofs | MAP | LaTex | BERT encodings |
| Welleck et al. (2021b) | $\begin{array}{l}\text { Next-step Suggestion } \\ \text { FEl Prof Gention }\end{array}$ | s | BART encoder with denoising pre-rraining | Naturalproofs | SBleu, Meteor, Edit, P, R, F1 | LaTeX | BART encodings pre-trained with denoising task |

Table 1: Summary of different methodologies for addressing tasks related to informal mathematical text. The methods are categorised in terms of (i) Task; (ii) Learning: Supervised (S), Self-supervised (SS), Unsupervised (UNS), Rule-based (R) (no learning); (iii) Approach; (iv) Dataset; (v) Metrics: MAP (Mean Average Precision), P@K (Precision at K), Perplexity, P (Precision), R (Recall), F1, Acc (Accuracy), BLEU, METEOR, MRR (Mean Reciprocal Rank), Edit (edit distance); (vi) Math format: MathML, LaTeX, natural language, or formal library (HOL, Mizar, Flyspeck, TPTP); (vii) Key representation of input text crucial to the approach.

> [!blank-container]
> 
> ![[figure_2.png]]
> 
> > **Figure 2**: Taxonomy for approaches related to identifier-definition extraction. "Intra-doc" and "extra-doc" refers to how identifiers and definitions are scoped from supporting text.



Identifier-definition extraction limitations. Methods considering the specific link between identifiers and their definitions have split off into at least three recent tasks: identifier-definition extraction (Schubotz et al., 2017; Alexeeva et al., 2020), variable typing (Stathopoulos et al., 2018), and notation auto-suggestion (Jo et al., 2021). A lack of consensus on the framing of the task and data prevents a direct comparison between methods. Schubotz et al. (2017) advise against using their gold standard data for training due to certain extractions being too difficult for automated systems, among other reasons. They also propose future research should focus on recall due to current methods extracting exact definitions for only $1 / 3$ of identifiers, and suggest use of multilingual semantic role labelling (Akbik et al., 2016) and logical deduction (Schubotz et al., 2016b). Logical deduction is partially tackled by Alexeeva et al. (2020), which is based on an open-domain causal IE system (Sharp et al., 2019) with Odin grammar (Valenzuela-Escárcega et al., 2016), where temporal logic is used to obtain intervals referred to by pre-identified time expressions (Sharp et al., 2019). We assume the issues with superscript identifiers (such as Einstein notation etc.) from Schubotz et al. (2016b) carry over into Schubotz et al. (2017). The rule-based approach proposed by Alexeeva et al. (2020) attempts to account for such notation (known as wildcards in formula retrieval). They propose future methods should combine grammar with a learning framework, extend rule sets to account for coordinate constructions, and create well-annotated training data using tools such as PDFAlign and others (Asakura et al., 2021).

### 2.2 Formula Retrieval

> [!definition] 
> Formula retrieval is a task in the field of information retrieval that involves finding and returning mathematical formulae from a database or corpus that match a given query. The query typically consists of a target mathematical formula, possibly expressed in a markup language like MathML, along with several related keywords. The goal is to return a ranked list of relevant retrieval units (such as documents, articles, or specific sections) that contain formulae matching the query. This task is crucial in scientific and academic research where mathematical formulae play a significant role.


> [!summary] 
> Math information retrieval (MIR) systems, such as those employed in the NTCIR11/12 MathIR Wikipedia Formula Retrieval/Browsing Tasks, have evolved to leverage both symbol layout trees (SLTs) and operator trees (OPTs) to improve formula retrieval. Models like Tangent-S and Tangent-CFT successfully integrate these two tree representations, while MathBERT focuses on encoding OPTs and using targeted objectives to account for mathematical text aspects. While such systems have improved, challenges persist, such as the need for better support for wildcard queries and more diverse formulae in testing, as well as improved recall through math synonym query expansion.


We discuss approaches related to the NTCIR11/12 MathIR Wikipedia Formula Retrieval/Browsing Tasks (Zanibbi et al., 2016a). Similar to NTCIR-11, the NTCIR-12 MathIR Task objective is to build math information retrieval (MIR) systems that <mark style="background: #ADCCFFA6;">enable users to search for a particular math concept using math formulae</mark>. Given a query which contains a target formula expressed in MathML and several related keywords, each participating system in this task is expected to return a ranked list of the relevant retrieval units containing formulae matching the query (Kristianto et al., 2016).

Combining formula tree representations improves retrieval. Two main tree representations of formulae exist: Symbol Layout Trees (SLTs) and Operator Trees (OPTs), shown in Figure 4.

Approaches reliant solely on SLTs, such as the early versions of the Tangent retrieval system (Pattaniyil and Zanibbi, 2014; Zanibbi et al., 2015, 2016b), or solely OPTs (Zhong and Zanibbi, 2019; Zhong et al., 2020) tend to return less relevant formulae from queries. OPTs capture formula semantics while SLTs capture visual structure (Mansouri et al., 2019). Effective representation of both formula layout and semantics within a single vector allows a model exploit both representations. Tangent-S (Davila and Zanibbi, 2017) was the first evolution of the Tangent system to outperform the NTCIR-11 (Aizawa et al., 2014) overall best performer, MCAT (Kristianto et al., 2014b, 2016), which encoded path and sibling information from MathML Presentation (SLT-based) and Content (OPT-based). Tangent-S jointly integrated SLTs and OPTs by combining scores for each representation through a simple linear regressor. Later, Tangent-CFT (Mansouri et al., 2019) considered SLTs and OPTs through a fastText (Bojanowski et al., 2017) n-gram embedding model using tree tuples. MathBERT (Peng et al., 2021) does not explicitly account for SLTs. They claim that LaTeX codes account for SLTs to some extent and there- fore focus on encoding OPTs. They pre-train the BERT (Vaswani et al., 2017) model with targeted objectives each accounting for different aspects of mathematical text. They account for OPTs by concatenating node sequences to formula + context BERT input sequences, and by formulating OPTbased structure-aware pre-training tasks learned in conjunction with masked language modelling (MLM).

> [!blank-container]
> ![[figure_3.png]]
> Figure 3: Taxonomy for approaches related to formula retrieval (math information retrieval). In the "SLT + OPT" (top right) the asterisks in MCAT* and MathBERT* refer to how SLTs and/or OPTs are not encoded directly from trees as seen in any of the Tangent approaches or Approach0. MCAT encodes SLTs implicitly through consideration of MathML Presentation, and OPTs through MathML Content. MathBERT encodes OPT tree information but implicitly encodes SLT informaton through LaTeX formulae, similarly to MCAT. The number in the bottom right of the lower-most boxes is the harmonic mean of partial and full Bpref@ 1000.


> [!blank-container]
> ![[figure_4.png]]
> 
> Figure 4: Formula (a) $y=e^{x}$ with its Symbol Layout Tree (SLT) (b), and Operator Tree (OPT) (c). SLTs represent formula appearance by the spatial arrangements of math symbols, while OPTs define the mathematical operations represented in expressions.


Leaf-root path tuples deliver an effective mechanism for embedding relations between symbol pairs. Leaf-root path tuples are now ubiquitous in formula retrieval (Zanibbi et al., 2015, 2016b; Davila and Zanibbi, 2017; Zhong and Zanibbi, 2019; Mansouri et al., 2019; Zhong et al., 2020) and their use for NTCIR-11/12 retrieval has varied since their conception (Stalnaker and Zanibbi, 2015). Initially (Pattaniyil and Zanibbi, 2014) pair tuples were used within a TF-IDF weighting scheme, then Zanibbi et al. (2015, 2016b) proposed an appearance-based similarity metric using SLTs, maximum subtree similarity (MSS). OPT tuples are integrated (Davila and Zanibbi, 2017) later on. Mansouri et al. (2019) treat tree tuples as words, extract n-grams, and learn fastText (Bo- janowski et al., 2017) formula embeddings. Zhong and Zanibbi (2019); Zhong et al. (2020) forgo machine learning altogether with an OPT-based heuristic search (Approach0) through a generalisation of MSS (Zanibbi et al., 2016b). Leaf-root path tuples effectively map symbol-pair relations and account for formula substructure, but there is dispute on how best to integrate them into existing machine learning or explicit retrieval frameworks. There exists contest between well-developed similarity heuristics (Zhong and Zanibbi, 2019) and embedding techniques (Mansouri et al., 2019), despite their complementarity.

Purely explicit methods still deliver competitive results. Tangent-CFT (Mansouri et al., 2019) and MathBERT (Peng et al., 2021) are two models to employ learning techniques beyond the level of linear regression. Each model is integrated with Approach0 (Zhong and Zanibbi, 2019) through the linear combination of individual model scores. This respectively forms the TanApp and MathApp baselines, the state-of-the-art in formula retrieval for non-wildcard queries. Approach0 achieves the highest full Bpref score (Peng et al., 2021) of the individual models, and highlights the power of explicit methods.

Formula retrieval limitations. Zhong and Zanibbi (2019) propose supporting query expansion of math synonyms to improve recall, and note that Approach0 does not support wildcard queries. Zhong et al. (2020) later provides basic support for wildcards. Tangent-CFT also does not evaluate on wildcard queries, and the authors suggest extending the test selection to include more diverse formulae, particularly those that are not present as exact matches. They propose integrating nearby text into learned embeddings. MathBERT (Peng et al., 2021) performs such integration, but does not learn n-gram embeddings. MathBERT evaluates on non-wildcard queries only.

### 2.3 Informal Premise Selection


> [!definition] Information Premise Selection
> Informal premise selection is an emerging field in automated theorem proving, where the goal is to select relevant statements or premises, written in natural language or LaTeX, that can be used to prove a given conjecture. Unlike formal premise selection which involves premises written in formal languages compatible with automated theorem provers, informal premise selection deals with premises in informal language that are not directly compatible with theorem provers. This task has been formulated as either a classification problem or a ranking problem. Current approaches consider both language and mathematics as a whole without separating the two modalities, which can limit performance.


> [!summary] 
> Premise selection, both formal and informal, is a process in automated theorem proving that involves choosing relevant statements to support a given conjecture. Formal premise selection aids in reducing the combinatorial search space to make theorem proving more tractable, while informal premise selection, a newer field, deals with premises written in natural language and LaTeX, aiming to retrieve premises most likely to be useful for proving a conjecture. Current research suggests that separate mechanisms for representing mathematics and natural language can improve performance, but limitations include a lack of structural consideration of formulae and limited variable typing capabilities.


Formal and informal premise selection both involve the selection of relevant statements that are useful for proving a given conjecture (Irving et al., 2016; Wang et al., 2017; Ferreira and Freitas, 2020a). The difference lies in the language from which the premises and related proof elements are composed, and their compatibility with existing Automated Theorem Provers (ATPs). Informal language is not compatible with existing provers without autoformalisation (Wang et al., 2020); a current bottleneck (Irving et al., 2016). Typically, when reasoning over large formal libraries comprising thousands of premises, the performance of ATPs degrades considerably, while for a given proof only a fraction of the premises are required to complete it (Urban et al., 2010; Alama et al., 2014). Theorem proving is essentially a search problem with a combinatorial search space, and the goal of formal premise selection is to reduce the space, making theorem proving tractable (Wang et al., 2017). While formal premises are written in the languages of formal libraries such as Mizar (Rudnicki, 1992), informal premises (and theorems) as seen in ProofWiki[^2] are written in combinations of natural language and LaTeX (Ferreira and Freitas, 2020a; Welleck et al., 2021a). Proposed approaches either rank (Han et al., 2021) or classify (Ferreira and Freitas, 2020b, 2021) candidate premises for a given proof, detached from formal libraries and ATPs. Informal premise selection is a recently emerging field. Figure 1 describes it as a mid-spectrum task between retrieval and abstraction. Premise selection models select from existing text without explicitly reasoning beyond it. However, proficient models may be somewhat logical by proxy through the very nature of selecting premises for mathematical reasoning chains. An example of informal premise selection is expressed through the natural language premise selection task, where, given a new conjecture $c$ that requires a mathematical proof and a collection (or knowledge base) of premises $P=\left\{p_{1}, p_{2}, \ldots, p_{N_{p}}\right\}$, with size $N_{p}$, the goal is to retrieve premises most likely to be useful for proving $c$ (Ferreira and Freitas, 2020a,b). This is formulated as a classification problem. Alternatively, Welleck et al. (2021a) propose mathematical reference retrieval as an analogue of premise selection. The goal is to retrieve the set of references (theorems, lemmas, definitions) that occur in its proof, formulated as a ranking problem (retrieval).

[^2]: https://proofwiki.org/wiki/Main_Page 

Separate mechanisms for representing mathematics and natural language can improve performance. Regardless of the task variation, current approaches (Ferreira and Freitas, 2020b; Welleck et al., 2021a; Han et al., 2021; Coavoux and Cohen, 2021) tend to jointly consider mathematics and language as a whole, not specifically accounting for aspects of each modality. Leading approaches for formula retrieval (Peng et al., 2021; Mansouri et al., 2019) or solving math word problems (Kim et al., 2020; Liang et al., 2021; Zhang et al., 2022) do not follow this trend. Ferreira and Freitas (2020b) extract a dependency graph representing dual-modality mathematical statements as nodes, and formulate the problem as link prediction (Zhang and Chen, 2018) similar to variable typing (Stathopoulos et al., 2018). Other transformer-based or self-attentive baselines (Ferreira and Freitas, 2020b; Welleck et al., 2021a; Han et al., 2021; Coavoux and Cohen, 2021) also do not separate mathematical elements from natural language. They consider notation with the same depth as word-level tokens and encode them similarly. Research in neuroscience (Butterworth, 2002; Amalric and Dehaene, 2016) suggests the brain handles mathematics separately to natural language: approaches in premise selection (Ferreira and Freitas, 2021) and other tasks (Peng et al., 2021; Zhang et al., 2022) have prospered from encoding mathematics through a separate mechanism to that of natural language. Ferreira and Freitas (2021) purposefully separate the two modalities, encoding each using self-attention and combining them with a bidirectional LSTM. Explicit disentanglement of the modalities forces the model to exploit latent relationships between language and mathematics through the LSTM layer.

###### Informal premise selection limitations. 
Limitations involve a lack of structural consideration of formulae and limited variable typing capabilities. Ferreira and Freitas (2020b) note that the graphbased approach to premise selection as link prediction struggles to encode mathematical statements which are mostly formulae, and suggest inclusion of structural embeddings (e.g. MathBERT (Peng et al., 2021)) and training BERT on a mathematical corpus. They also describe value in formulating sophisticated heuristics for navigating the premises graph. Later, following a Siamese network architecture (Ferreira and Freitas, 2021) reliant on dual-layer word/expression self-attention and a BiLSTM (STAR), the authors demonstrate that STAR does not appropriately encode the semantics of variables. They suggest that variable typing and representation are a fundamental component of encoding mathematical statements. Han et al. (2021) plan to explore the effect of varying pre-training components, testing zero-shot perfor- mance without contrastive fine-tuning, and unsupervised retrieval. Coavoux and Cohen (2021) propose a statement-proof matching task akin to informal premise selection, with a solution reliant on a self-attentive encoder and bilinear similarity function. The authors note model confusion due to the proofs introducing new concepts and variables rather than referring to existing concepts.

### 2.4 Math Word Problems


> [!definition] Math Word Problems
> Math word problems refer to mathematical problems that are presented in a narrative format, requiring the reader to discern the relevant numbers, operations, and relationships from the text to solve the problem. The problem typically describes a real-life situation, and the solver must translate this textual information into a mathematical equation or set of equations that can be solved to find the answer. Solving math word problems involves skills like comprehension of the text, identification of the problem's variables and relationships, and application of relevant mathematical operations.


> [!summary] 
> Math word problem solving, a task that involves translating a textual paragraph into solvable equations, has seen recent advancements in the use of dependency graphs and multi-encoder/multi-decoder architectures for improved performance. Graph-based approaches construct various kinds of graphs representing the problem's linguistic and numerical aspects, while multi-encoder and multi-decoder architectures help in combining complementary problem representations. The field still faces limitations, such as needing to consider more complex relations between quantities and language, handling fragmentation issues, integrating external knowledge sources, and developing real-world datasets for unsupervised or weakly supervised approaches.

Solving math word problems dates back to the dawn of artificial intelligence (Feigenbaum et al., 1963; Bobrow, 1964; Charniak, 1969). It can be defined as the task of translating a paragraph into a set of equations to be solved (Li et al., 2020). We focus on trends in the task since 2019, as a detailed survey (Zhang et al., 2019) captures prior work.

Use of dependency graphs are instrumental to support inference. In graph-based approaches to solving MWPs, embeddings of words, numbers, or relationship graph nodes, are learned through graph encoders which feed information through to tree (or sequence) decoders. Embeddings are decoded into expression trees which determine the problem solution. Li et al. (2020) learn the mapping between a heterogeneous graph representing the input problem, and an output tree. The graph is constructed from word nodes with relationship nodes of a parsing tree. This is either a dependency parse tree or constituency tree. Zhang et al. (2020) represent two separate graphs: a quantity cell graph associating descriptive words with problem quantities, and a quantity comparison graph which retains numerical qualities of the quantity, and leverages heuristics to represent relationships between quantities such that solution expressions reflect a more realistic arithmetic order. Shen and Jin (2020) also extract two graphs: a dependency parse tree and numerical comparison graph. Zhang et al. (2022) construct a heterogeneous graph from three subgraphs: a wordword graph containing syntactic and semantic relationships between words, a number-word graph, and a number comparison graph. Their model is the best performing graph-based approach to date. Although other important differences exist (such as decoder choice), it seems that models explicitly relating multiple linguistic aspects of problem text tend to deliver better problem solving.



> [!blank-container]
> ![[figure_5.png]]
> Figure 5: Taxonomy for methods related to math word problem solving.

Multi-encoders and multi-decoders improve performance by combining complementary representations. Another impactful architectural decision is the choice of encoder/decoder. To highlight this, we consider the following comparison. Shen and Jin (2020) and Zhang et al. (2020) each extract two graphs from the problem text. One is a number comparison graph, and the other relates word-word pairs (Shen and Jin, 2020) or word-number pairs (Zhang et al., 2020). They both encode two graphs rather than one heterogeneous graph (Li et al., 2020; Zhang et al., 2022). They both use a similar tree-based decoder (Xie and Sun, 2019). A key difference is that Shen and Jin (2020) includes an additional sequencebased encoder and decoder. The sequence-based encoder first obtains a textual representation of the input paragraph, then the graph-based encoder integrates the two encoded graphs. Then treebased and sequence-based decoders generate different equation expressions for the problem with an additional mechanism for optimising solution expression selection. In their own work, Shen and Jin (2020) demonstrate the impact of multiencoders/decoders over each encoder/decoder option individually through ablation.

Goal-driven decompositional tree-based decoders are a significant component in the stateof-the-art. Introduced in Xie and Sun (2019), this class of decoder is considered by all but three discussed models, as shown in Figure 5 and ex- tends to non-graph-based models (Qin et al., 2021; Liang et al., 2021). In GTS, goal vectors guide construction of expression subtrees (from token node embeddings) in a recursive manner, until a solution expression tree is generated. Proposed models do expand on the GTS-based decoder through the inclusion of semantically-aligned universal expression trees (Qin et al., 2020, 2021), though this adaptation is not as widely used. The state-of-the-art (Liang et al., 2021; Zhang et al., 2022) approaches follow the GTS decoder closely. Language models that transfer knowledge learned from auxiliary tasks rival models based on explicit graph representation of problem text. As an alternative to encoding explicit relations through graphs, other work (Kim et al., 2020; Qin et al., 2021; Liang et al., 2021) relies on pre-trained transformer-based models, and those which incorporate auxiliary tasks assumed relevant for solving MWPs to latently learn such relations. However, it seems the case that auxiliary tasks alone do not deliver competitive performance (Qin et al., 2020) without the extensive pretraining efforts with large corpora, as we see with BERT-based transformer models. These use either both the (ALBERT (Lan et al., 2019)) encoder and decoder (Kim et al., 2020), or BERT-based encoder with goal-driven tree-based decoder (Liang et al., 2021). Math word problem limitations. In Graph2TreeZ (Zhang et al., 2020), they suggest considering more complex relations between quantities and language, and introducing heuristics to improve solution expression generation from the tree-based decoder. In EPT, Kim et al. (2020) find error probability related to fragmentation issues increases exponentially with number of unknowns, and propose generalising EPT to other MWP datasets. HGEN (Zhang et al., 2022) note three areas of future improvement: Combining models into a unified framework through ensembling multiple encoders (similar to (Ferreira and Freitas, 2021)); integrating external knowledge sources (e.g. HowNet (Dong and Dong, 2003), Cilin (Hong-Minh and Smith, 2008)); and realworld dataset development for unsupervised or weakly supervised approaches (Qin et al., 2020).

### 2.5 Informal Theorem Proving


> [!definition] Informal Theorem Proving
> Informal theorem proving refers to the process of mathematical reasoning and proof generation that is not strictly formalized. It involves creating and understanding mathematical proofs in natural language or semi-formal language, rather than in a strictly formal language that a machine could interpret. It's a significant area of study in artificial intelligence, as it involves translating complex, human-like reasoning into a format that a machine can process and utilize. This is typically more flexible than formal theorem proving, but also less rigorous.


> [!summary] 
> This passage discusses two major challenges in automated theorem proving: autoformalisation (translating informal mathematical text into formal language), and developing robust reasoning methods for formalised proofs. Autoformalisation is considered extremely difficult, and strategies range from direct translation of mathematical text to a combination of exploration and approximate translation using formula embeddings. The text also emphasizes the need for developing robust, interactive natural language theorem provers, along with the associated challenges and limitations.

Formal automated theorem proving in logic is among the most advanced and abstract forms of reasoning materialised in the AI space. There are two major bottlenecks (Irving et al., 2016) formal methods must overcome: (1) translating informal mathematical text into formal language (autoformalisation), and (2) a lack of strong automated reasoning methods to fill in the gaps in already formalised human-written proofs. Informal methods either tackle autoformalisation directly (Wang et al., 2020; Wu et al., 2022), or circumvent it through language modelling-based proof generation (Welleck et al., 2021a,b), trading formal rigour for flexibility. Transformer-based models have been proposed for mathematical reasoning (Polu and Sutskever, 2020; Rabe et al., 2020; Wu et al., 2021). Converting informal mathematical text into forms interpretable by computers (Kaliszyk et al., 2015a,b; Szegedy, 2020; Wang and Deng, 2020; Meadows and Freitas, 2021) is closer to the real-world reasoning and communication format followed by mathematicians.

Autoformalisation could be addressed through approximate translation and exploration rather than direct machine translation. A long-studied and extremely challenging endeavour (Zinn, 1999, 2003); autoformalisation involves converting informal mathematical text into language interpretable by theorem provers (Kaliszyk et al., 2015b; Wang et al., 2020; Szegedy, 2020). Kaliszyk et al. (2015b) propose a statistical learning approach for parsing ambiguous formulae over the Flyspeck formal mathematical corpus (Hales, 2006). Later, thanks to improved machine translation capabilities (Luong et al., 2017; Lample et al., 2018; Lample and Conneau, 2019), Wang et al. (2020) explore dataset translation experiments between LaTeX code extracted from ProofWiki, and formal libraries Mizar (Rudnicki, 1992) and TPTP (Sutcliffe and Suttner, 1998). The supervised RNN-based neural machine translation model (Luong et al., 2017) outperforms the transformer-based (Lample et al., 2018) and MLM pre-trained transformer-based (Lample and Conneau, 2019) models, with the performance boost stemming from its use of alignment data. Szegedy (2020) advises against such direct translation efforts, instead proposing a combination of exploration and approximate translation through predicting formula embeddings. In seq $2 \mathrm{seq}$ models, embeddings are typically granular, encoding word-level or symbol-level (Jo et al., 2021) tokens. The suggestion is to learn mappings from natural language input to premise statements nearby the desired statement in the embedding space, traversing the space between statements using a suitable prover (Bansal et al., 2019). Guided mathematical exploration for real-world proofs is still an unaddressed problem and does not scale well with step-distance between current and desired formulae. It may be easier to continue with direct translation (Wang et al., 2020). For example, Wu et al. (2022) report promising results, directly autoformalising small competition problems to Isabelle statements using language models. Similar to previous suggestion (Szegedy, 2020), they also autoformalize statements as targets for proof search with a neural theorem prover.

Need for developing robust interactive natural language theorem provers. We discuss the closest equivalent to formal theorem proving in an informal setting. Welleck et al. (2021a) propose a mathematical reference generation task. Given a mathematical claim, the order and number of references within a proof are predicted. A reference is a theorem, definition, or a page that is linked to within the contents of a statement or proof. Each theorem $\mathbf{x}$ has a proof containing a sequence of references $\mathbf{y}=\left(\mathbf{r}_{1}, \ldots, \mathbf{r}_{|\mathbf{y}|}\right)$, for references $\mathbf{r}_{\mathrm{m}} \in \mathcal{R}$. Where the retrieval task assigns a score to each reference in $\mathcal{R}$, the generation task produces a variable length of sequence of references $\left(\hat{\mathbf{r}}_{1}, \ldots, \hat{\mathbf{r}}_{|\mathbf{y}|}\right)$ with the goal of matching $\mathbf{y}$, for which a BERT-based model is employed and fine-tuned on various data sources. Welleck et al. (2021b) expand on their proof generation work, proposing two related tasks: next-step suggestion, where a step from a proof $\mathbf{y}$ (as described above) is defined as a sequence of tokens to be generated, given the previous steps and the claim $x$; and full-proof generation which extends this task to generate the full proof. They employ BART (Lewis et al., 2019), an encoder-decoder model pre-trained with denoising tasks, and augment the model with reference knowledge using Fusion-in-Decoder (Izacard and Grave, 2020). The intermediate denoising training and knowledge-grounding improve model performance by producing better representations of (denoised) references for deployment at generation time, and by encoding reference-augmented inputs. Aiming towards automatic physics derivation, Meadows and Freitas (2021) propose an equation reconstruction task similar to next-step suggestion, where, given a sequence of LaTeX strings $\left\{s_{i-1}, s_{i}, s_{i+1}\right\}$ from a computer algebra physics derivation $\left\{s_{1}, \ldots, s_{N}\right\}$, the intermediate string $s_{i}$ is removed, and must be re-derived. The similarity-based heuristic search selects two consecutive computer algebra operations from a knowledge base and sequentially applies them to $s_{i-1}$, in order to derive the known equation $s_{i+1}$. If $s_{i+1}$ is obtained, then the equation after the first operation is taken as $s_{i}$, and a partial derivation is achieved.

Informal theorem proving limitations. Wang et al. (2020) suggest the development of highquality datasets for evaluating translation models, including structural formula representations, and jointly embedding multiple proof assistant libraries to increase formal dataset size. Szegedy (2020) argues that reasoning systems based on self-driven exploration without informal communication capabilities would suffer usage and evaluation difficulties. Wu et al. (2022) note limitations with text window size and difficulty storing large formal theories with current language models. After proposing the NaturalProofs dataset, Welleck et al. (2021a) characterize error types for the full-proof generation and next-step suggestion tasks, noting issues with: (1) hallucinated references, meaning the reference does not occur in NaturalProofs; (2) non-ground-truth reference, meaning the reference does not occur in the ground-truth proof; (3) undefined terms; and (4) improper or irrelevant statement, meaning a statement that is mathematically invalid (e.g. $2 / 3 \in \mathbb{Z}$ ) or irrelevant to the proof; and (5) statements that do not follow logically from the preceding statements. Dealing with research-level physics, Meadows and Freitas (2021) note that the cost of semi-automated formalisation is significant and does not scale well, requiring detailed expert-level manual intervention. They also call for a set of well-defined computer algebra operations such that robust mathematical exploration can be guided in a goal-based setting.

## 3 Conclusion

In this work we deliver a synthesis of the recent evolutionary arch for strategic areas in mathematical language processing. We systematically describe the methods, challenges and trends within each area, eliciting consolidated modelling components and emerging methodological advances. In areas related to variable typing and formula retrieval, explicit methods compete with and complement embedding models. In word problem solving involving simpler mathematics, dependency graphs explicitly represent relationships between numerical tokens and language. Models either encode graph input or sequence input, and decode to solution expression trees via recursive goal-driven tree decoders. Research with multi-encoders/decoders suggests value in combining representations. For advanced mathematics, language-based premise selection models also use graph-based and transformer-based models, mostly learning formulae and language embeddings without integrating formula structure or variable typing. Limited autoformalisation of informal mathematics exists through machine translation, but it is elsewhere argued that approximate translation to related premises followed by exploration is more promising. Some circumvent formal libraries altogether through flexible proof generation, physics derivation, and premise selection in less formal environments. We hope future techniques will benefit from this synthesis.


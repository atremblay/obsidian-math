
B Time-Mixing Block as an RNN Cell

As stated in 4.3, the RWKV time-mixing block can
be formulated as an RNN, as the W KV computa-
tion can be written in such a recursive form:

a0, b0 = 0,

wkvt =

at−1 + eu+ktvt
bt−1 + eu+kt

,

at = e−wat−1 + ektvt,
bt = e−wbt−1 + ekt.

(19)

(20)

(21)

(22)

The dataﬂow of the RNN-like time-mixing is
shown in Fig. 7, where the hidden states h is the
numerator-denominator tuple (a, b).

Figure 7: RWKV time-mixing block formulated as an
RNN cell. Color codes: yellow (µ) denotes the token
shift, red (1) denotes the denominator, blue (2) denotes
the numerator, pink (3) denotes the fraction computa-
tions in 14. h denotes the numerator-denominator tuple
(a, b).

To avoid overﬂow in calculating ekt, a numerical
trick is used in the ofﬁcial implementation. Note

a1 = e−wa0 + ek0v0 = ek0v0,
b1 = e−wb0 + ek0 = ek0,

(23)

(24)

1 = v0, b(cid:48)

and we set a(cid:48)
1 = 1, p0 = k0, where pt−1
stores the shared exponents of at and bt. Now the
above recursion can be converted into a numerical
safe version, for each time step t > 1:

q := max(pt−1, u + kt),
t = ept−1−qa(cid:48)
a∗
t = ept−1−qb(cid:48)
b∗
a∗
t
b∗
t

t−1 + eu+kt−qvt,
t−1 + eu+kt−q,

.

wkvt =

(25)

(26)

(27)

(28)

The update to a(cid:48)
t, b(cid:48)
also carried out in similar fashion:

t and their shared exponent are

q := max(pt−1 − w, kt),
a(cid:48)
t = ept−1−w−qa(cid:48)
t = ept−1−w−qb(cid:48)
b(cid:48)
pt = q.

t−1 + ekt−qvt,
t−1 + ekt−q,

(29)

(30)

(31)

(32)

C Parameter and FLOP Count for the

RWKV Models

The following section provides an overview of the
different RWKV model architectures along with
their respective parameter and FLOP counts in Ta-
ble 2.

Name, Layers, Model Dimension, Parameters, FLOPs per token

169 M
430 M
1.5 B
3 B
7 B
14 B

12
24
24
32
32
40

768
1024
2048
2560
4096
5120

1.693 × 108
4.304 × 108
1.515 × 109
2.985 × 109
7.393 × 109
1.415 × 1010

2.613 × 108
7.573 × 108
2.823 × 109
5.710 × 109
1.437 × 1010
2.778 × 1010

Table 2: RWKV model architectures and associated
FLOP counts

The number of parameters for each model is
computed using the formula: #parameters =
2V D + 13D2L + D(11L + 4) where V = 50277
is the vocabulary size, D represents the Model Di-
mension and L corresponds to the number of lay-
ers.

FLOPs is for a forward pass for one token. It
was calculated as 6(V D + 13D2L), which is the
twice (add and multiply) the number of parameters
in linear layers. The backwards pass FLOPs can be
approximated as twice that of the forward pass. So

12e3the total is 6(V D + 13D2L) per token for training
(3x fw FLOPs). It is noteworthy that FLOPs are
independent of the context length, unlike regular
transformers. The FLOP approximations in this
paper are in line with the methodology used by
Kaplan et al. (2020).

Alternative approximations for FLOPs include
doubling the parameters which yields similar re-
sults within 2% for 14B and a 30% discrepancy for
169M variant. Another approximation is based on
the number of non-embedding parameters multi-
plied by 2. This gives 2(V D + 13D2L + D(11L +
4)) resulting in 1.6% more FLOPs for 14B model
and 8% more FLOPs for 169M model.

D Parameter initializations

We describe the speciﬁc parameter initializations
below and motivate the design choices. Parame-
ters belonging to residual blocks are often adjusted
by layer depth and total number of layers. Let #
denote the vocabulary size, s denote the embed-
ding dimension, d denote the hidden size (we use
d = 4s), L the number of layers, l the layer index
(from 0 to L-1), we use the following initializa-
tions:

• Embeddings are initialized to U(±1e-4) as

explained in 4.7

• All LayerNorm weights start from 1 and bi-

ases from 0.



• For the channel-mixing blocks (11), µki and

µri are initialized to ( i

s )1− l

L

• For the time-mixing blocks (16), initializa-
L + 0.3l
L−1

L , µvi = ( i

s )1− l

tions are µki = ( i
and µri = 0.5( i

s )1− l
s )1− l

L

• wi (14), also known as “time decay”, is initial-
d−1 )0.7+ 1.3l
ized to −5+8·( i
L−1 . Intuitively, it is
the discount factor applied to previous tokens
over time.

• ui (14), also known as “bonus”, is set to
0.5(((i + 1) mod 3) − 1) + log 0.3.
It is
the special weighting applied to the current
token in equation 14. The alternating zigzag
pattern initially creates subtle variations in the
tensor elements, which are intended to help
the model treat different dimensions of the
embedding distinctively.

• Wo (15) (time-mixing) and Wv (channel-

mixing) are initialized to N (0,(cid:113) d

s = 2)

• All Wr, Wk, Wv weights are initialized to 0
so the model can start learning from the be-
ginning without noisy signals.



• All LayerNorm weights start from 1 and bi- ases from 0.


E Small Init Embedding

This section presents experimental validation of
small initialization embedding. The experimental
setup is as follows. In the baseline conﬁguration,
the parameters are initialized using a normal distri-
bution with a mean of 0.0 and a standard deviation
of 0.02, which is a commonly used initialization
method in models like BERT and GPT. On the other
hand, in the small initialization of the embedding
(small init emb) experiment, the parameters are ini-
tialized using a uniform distribution with a range of
1e-4, which is slightly different from RWKV where
a normal distribution with a standard deviation of
1e-4 is used. However, this difference is negligible
and does not affect our conclusions. The experi-
ments were conducted with a batch size of 400. As
depicted in the ﬁgure 8, the loss curve for the small
init emb exhibits a faster rate of decrease and con-
vergence compared to the traditional initialization
using a normal distribution.

Figure 8: Effect of small initialization embedding.

F Gradient Stability in RWKV

In this section, we present a mathematical descrip-
tion of the gradient stability property in RWKV,
focusing speciﬁcally on the time-mixing block. By
gradient stability we mean that if the inputs xt
are bounded and the model parameters are ﬁxed,
then the gradients with respect to Wk and Wv are
uniformly bounded for all T (thus not exploding).
Consequently, we can control the amount each xt
contributes to the gradient at T in a naturally de-
caying fashion by the weight decay mechanism w

01000020000300004000050000Step4567891011LossBaselineSmall Init Emb(thus not vanishing unless desired).

First, we make the simpliﬁcation that there are
no token shifts, this will not affect the ﬁnal conclu-
sion. In this scenario, wkvT can be written as

wkvT =

(cid:80)T

t=1 Ke
t vt
(cid:80)T
t=1 Ke
t

= E(vt) =

S(vt)
S(1)

,

(33)

where

vt = Wvxt,

∂(vt)i
∂(Wv)i,j

= (xt)j,

Ke

t = eWkxt+wT,t,

∂(Ke
t )i
∂(Wk)i,j

= (xt)j(Ke

t )i,

and S(·) and E(·) are shorthand for denoting sums
and averages over weights Ke
t .

The loss function at position T can be written as

LT = l(f (wkvT ), yT ).

(34)

Figure 9: Model behavior visualizations of the RWKV
model.

Because wkvT relates to (Wk)i,j and (Wv)i,j only
through the i-th channel (wkvT )i, we have

∂LT
∂(Wv)i,j

=

∂LT
∂(wkvT )i

∂(wkvT )i
∂(Wv)i,j

.

(35)

The ﬁrst part of above equation contains trivial
operations like output layers, and other layers of
time-mixing, which can be proven inductively. The
second part of above equation can be bounded as

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂(wkvT )i
∂(Wv)i,j

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂Ei[(vt)i]
∂(Wv)i,j

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= |Ei[(xt)j]| ≤ max

t

|(xt)j|,

(36)

which is irrelevant to T . Similarly,

∂(wkvT )i
∂(Wk)i,j

= ∂

Si[(vt)i]
Si(1)
Si[(xt)j(vt)i]
Si(1)

=

/∂(Wk)i,j

−

Si[(xt)j]Si[(vt)i]
Si(1)2

= Ei[(xt)j(vt)i] − Ei[(xt)j]Ei[(vt)i]
= covi((xt)j, (vt)i)
(37)

can also be bounded. Note that wkv’s softmax op-
eration contains at least two non-zero terms (u and
w), so the above “covariance” will not degenerate
into 0.

G Model Behavior Visualization

In Figure 9, we present visualizations of some be-
havior of the RWKV model.

The top plot illustrates the time decays (e−w) in
each layer of the RWKV-169M model, sorted along
the channel axis. Notably, several decays in the last
layers are very close or equal to one, implying that
certain information is preserved and propagated
throughout the model’s temporal context. Mean-
while, many decays in the initial layer are close
to zero, which corresponds to local operations in
wkv (14), likely to be associated with tasks such as
text parsing or lexical analysis. (Note that the local
operations in wkv is due to the extra parameter u,
when e−w is degenerated into 0.) These patterns of
time decays are partly learned, but also come from
parameter initialization as it speeds up training.

The bottom plot shows the information retrieval
and propagation path in the RWKV-430M model.
The experiment follows the causal trace method
introduced by Meng et al. (2022), where we

1. Run the model once, and record all states and
activation of each layer during the computa-
tion;

2. Corrupt the input embeddings of the subject
using noise (“The Eiffel Tower” in this exam-
ple);

0100200300400500600700800Channel0.00.20.40.60.81.0Time DecayTime decay (sorted along channel axis)Layer 1Layer 2Layer 3Layer 4Layer 5Layer 6Layer 7Layer 8Layer 9Layer 10Layer 11Layer 1216111621LayerThe Eiffel Tower is located in the city ofInformation propagation path7654321Log-probability of "Paris"3. Restore the states and activation of a certain
layer at a certain token during the compu-
tation, and record the log-probability of the
model outputting the correct answer (“Paris”).

Unlike transformers, RWKV relies on recursive
propagation of information in the time dimension.
In this case, the fact that "the Eiffel Tower is located
in Paris" is retrieved in layer 4. It is then passed
down to the subsequent layers. In layer 20, mostly,
the information is propagated through time until
reaching where it is needed. Finally, it is passed
down to the last layer for outputting the answer.

H Evaluation Details

The results for following tasks are in Table 3 and 4.

Tasks:

• LAMBADA (Paperno et al., 2016). A bench-
mark dataset that evaluates the model’s contex-
tual reasoning and language comprehension
abilities by presenting context-target pairs,
where the objective is to predict the most prob-
able target token.

• PIQA (Bisk et al., 2020). A benchmark for
the task of physical common sense reasoning,
which consists of a binary choice task that
can be better understood as a set of two pairs,
namely (Goal, Solution).

• HellaSwag (Zellers et al., 2019) A novel
benchmark for commonsense Natural Lan-
guage Inference (NLI) which is build by ad-
versarial ﬁltering against transformer models.
• Winogrande (Zellers et al., 2020) A dataset
designed to evaluate the acquisition of com-
mon sense reasoning by neural language mod-
els, aiming to determine whether we are ac-
curately assessing the true capabilities of ma-
chine common sense.

• StoryCloze (Mostafazadeh et al., 2016) A
benchmark to present a novel approach to as-
sess comprehension of narratives, narrative
generation, and script acquisition, focusing on
commonsense reasoning.

• ARC Challenge (Clark et al., 2018) A dataset
designed for multiple-choice question answer-
ing, encompassing science exam questions
ranging from third grade to ninth grade.

• ARC Easy An easy subset of ARC.
• HeadQA (Vilares and Gómez-Rodríguez,
2019) A benchmark consisting of graduate-
level questions encompassing various ﬁelds

such as medicine, nursing, biology, chemistry,
psychology, and pharmacology.

• OpenBookQA (Mihaylov et al., 2018) A QA
dataset to evaluate human comprehension of
a subject by incorporating open book facts,
scientiﬁc knowledge, and perceptual common
sense, drawing inspiration from open book
exams.

• SciQ (Johannes Welbl Nelson F. Liu, 2017)
A multiple-choice QA dataset which was cre-
ated using an innovative approach to gather
well-crafted multiple-choice questions that are
focused on a speciﬁc domain.

• TriviaQA (Joshi et al., 2017) A QA-IR dataset
which is constituted of triples of questions,
answers, supporting evidence, and indepen-
dently collected evidence documents, with an
average of six documents per question for re-
liable sources.

• ReCoRD (Zhang et al., 2018) A benchmark
for evaluating commonsense reasoning in
reading comprehension by generating queries
from CNN/Daily Mail news articles and re-
quiring text span answers from corresponding
summarizing passages.

• COPA (Roemmele et al., 2018) A dataset to
evaluate achievement in open-domain com-
monsense causal reasoning.

• MMMLU (Hendrycks et al., 2021) A multi-
task dataset for 57 tasks containing elementary
mathematics, US history, computer science,
law, etc.

I

Inference results

Figures 10 and 11 illustrate, respectively, the results
on time (s) and memory (RAM, VRAM) require-
ments for LLM inference in ﬂoat32 precision. We
benchmark the following model families and sizes:

• RWKV: 169m, 430m, 1.4b, 3b, 7b, 14b
• Bloom (Scao et al., 2022): 560m, 1b, 3b
• OPT (Zhang et al., 2022): 125m, 350m, 1.3b,

2.7b, 6.7b, 13b

• GPT-Neo (Black et al., 2022): 125m, 1.3b,

2.7b

• Pythia (Biderman et al., 2023): 160m, 410m,

1.4b, 2.8b, 6.7b, 12b

Missing models in are due to Out Of Memory
(OOM) errors. A comparison at 512 tokens is
shown in Figure 11 as some large transformer mod-
els produced an OOM when inferencing longer se-

Model

RWKV-4
Pythia
GPT-Neo

RWKV-4
Pythia
GPT-Neo

RWKV-4
Pythia
GPT-Neo

RWKV-4
Pythia
GPT-Neo

RWKV-4
Pythia
GPT-J

RWKV-4
GPT-level∗

Pythia (c.f.)
GPT-NeoX (c.f.)

Params
B

0.17
0.16
0.16

0.43
0.40
0.40

1.5
1.4
1.4

3.0
2.8
2.8

7.4
6.9
6.1

14.2
14.2

11.8
20.6

PIQA StoryCloze HellaSwag WinoGrande ARC-e ARC-c
acc

acc_norm

acc

acc

acc

acc_norm acc_norm

OBQA

65.07
62.68
63.06

67.52
66.70
65.07

72.36
71.11
71.16

74.16
73.83
72.14

76.06
74.54
75.41

77.48
76.49

75.90
77.69

58.79
58.47
58.26

63.87
62.64
61.04

68.73
67.66
67.72

70.71
70.71
69.54

73.44
72.96
74.02

76.06
74.97

74.40
76.11

32.26
31.63
30.42

40.90
39.10
37.64

52.48
50.82
48.94

59.89
59.46
55.82

65.51
63.92
66.25

70.65
68.72

67.38
71.42

50.83
52.01
50.43

51.14
53.35
51.14

54.62
56.51
54.93

59.59
61.25
57.62

61.01
61.01
64.09

63.85
65.14

64.72
65.98

47.47
45.12
43.73

52.86
50.38
48.91

60.48
57.74
56.19

65.19
62.84
61.07

67.80
66.79
66.92

70.24
70.77

69.82
72.69

24.15
23.81
23.12

25.17
25.77
25.34

29.44
28.58
25.85

33.11
32.25
30.20

37.46
35.07
36.60

38.99
37.99

36.77
40.44

29.60
29.20
26.20

32.40
30.00
30.60

34.00
30.80
33.60

37.00
35.20
33.20

40.20
38.00
38.20

41.80
39.27

38.80
40.20

Table 3: Zero-Shot Performance of the model on Common Sense Reasoning Tasks. ∗ Interpolation of Pythia and
GPT-Neo models

Model

RWKV-4
Pythia
GPT-Neo

RWKV-4
Pythia
GPT-Neo

RWKV-4
Pythia
GPT-Neo

RWKV-4
Pythia
GPT-Neo

RWKV-4
Pythia
GPT-J

RWKV-4
GPT-level∗

Pythia (c.f.)
GPT-NeoX (c.f.)

Params
B

0.17
0.16
0.16

0.43
0.40
0.40

1.5
1.4
1.4

3.0
2.8
2.8

7.4
6.9
6.1

14.2
14.2

11.8
20.6

LAMBADA LAMBADA headQA
ppl

sciq
acc_norm acc

acc

triviaQA ReCoRD COPA
acc

acc

em

29.33
24.38
30.27

13.04
11.58
13.88

7.04
6.58
7.5

5.25
4.93
5.63

4.38
4.3
4.1

3.86
3.81

3.89
3.64

32.99
38.97
37.36

45.16
50.44
47.29

56.43
60.43
57.25

63.96
65.36
62.22

67.18
67.98
68.31

70.83
70.94

70.44
71.94

25.78
25.82
25.16

27.32
25.09
26.00

27.64
27.02
27.86

28.45
28.96
27.17

31.22
28.59
28.67

32.64
31.03

30.74
31.62

77.50
76.50
76.60

80.30
81.50
81.10

85.00
85.50
86.00

86.50
87.70
89.30

88.80
90.00
91.50

90.40
92.20

91.80
93.00

1.26
1.31
1.18

2.35
2.03
1.38

5.65
5.52
5.24

11.68
9.63
4.82

18.30
15.42
16.74

24.58
22.37

20.57
25.99

62.03
66.32
64.92

70.48
75.05
73.79

76.97
81.43
80.62

80.87
85.10
83.80

83.68
86.44
87.71

85.67
87.89

87.58
88.52

66.00
62.00
64.00

65.00
67.00
65.00

77.00
73.00
69.00

82.00
77.00
80.00

85.00
85.00
83.00

85.00
82.66

82.00
84.00

Table 4: Zero-Shot Performance of various models on different tasks. ∗ Interpolation of Pythia and GPT-Neo
models

Method

Transformer
Transformer

Reformer
Synthesizer
Linear Transformer
Performer
AFT-simple

RWKV-RNN

L

12
24

12
12
12
12
12

6

d

512
256

512
512
512
512
512

512

T

Train bpc

Test bpc

Time Complexity

Space Complexity

1024
1024

1024
1024
1024
1024
1024

0.977
1.039

1.040
0.994
0.981
1.002
0.854

1.137
1.130

1.195
1.298
1.207
1.199
1.180

O(T 2d)
O(T 2d)

O(T log T d)
O(T 2d)
O(T d2)
O(T d2 log d)
O(T d)

O(T 2 + T d)
O(T 2 + T d)

O(T log T + T d)
O(T 2 + T d)
O(T d + d2)
O(T d log d + d2 log d)
O(T d)

1024

0.720

-

O(Td)

O(d)

Table 5: Enwik8 results, measured in bits per character (bpc): the lower the better. Baseline comparisons are made
with Reformer (Kitaev et al., 2020), Synthesizer (Tay et al., 2020) (the best performing dense version), Linear
Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020). L, d, and T denote the number
of blocks (network depth), dimension of features, and sequence length, respectively. Both Linear Transformer and
Performer are implemented with customized CUDA kernels (github.com/idiap/fast-transformers), and all other
models are implemented in native Pytorch.

quences. For GPU experiments, we use an NVIDIA
A100 with 80GB of VRAM. For CPU experiments,
we use an AMD EPYC processor with 30 CPU
cores and 200 GiB RAM.

Figure 10: Text generation inference memory (CPU
RAM, GPU VRAM) for LLMs. Model parameters are
not accounted.

Figure 11: Text generation inference time for LLMs.

Task Name Measure

type

ChatGPT GPT-4 RWKV-4
[%] GPT [%]

[%]

RWKV-4 SOTA
[%]

changed [%]

F1 Macro
RTE
WNLI
Accuracy
GoEmotions F1 Macro
F1 Macro
PolEmo2

88.1
81.7
25.6
44.1

91.3
91.6
23.1
41.0

44.2
47.9
7.9
38.2

74.8
49.3
7.9
40.9

92.1
97.9
52.8
76.4

Table 6: ChatGPT, GPT-4 and RWKV-4-Raven-14B
reasoning performance comparison in RTE (Wang
et al., 2019), WNLI (Wang et al., 2018), GoEmotions
(Demszky et al., 2020), and PolEmo2 (Koco´n et al.,
2019) benchmarks. SOTA is provided as a supplemen-
tary reference.

J

Importance of prompt construction
and comparison to GPT models

Inspired by article (Koco´n et al., 2023), we com-
pared the zero-shot performance of the RWKV-
4-Raven-14B with ChatGPT (access in February
2023) and GPT-4 using several known NLP tasks,
i.e., recognizing textual entailment (RTE), Wino-
grad Natural Language Inference (WNLI), and rec-
ognizing emotions elicited in readers (GoEmotions
and PolEmo2). Each model got the same prompts
manually chosen to receive proper responses from
the ChatGPT model. As shown in Tab. 6, RWKV
performs signiﬁcantly worse than ChatGPT and
GPT-4 in speciﬁc task performance. We suspect
that this disparity is likely caused by the choice
of prompts used to generate the answers. Given
that prompts are in natural language and do not
consider that RWKV is an RNN, so it can not look
back inside an instruction.

When the instruction style was adapted to re-
spect that RNNs is not capable for retrospective
processing, quality on some datasets increased sig-
niﬁcantly (ex.
for RTE (Wang et al., 2019) F1
Macro increased from 44.2% to 74.8%). We hy-
pothesize that RWKV models are more sensitive
to the position of the components in the context,
as RNN-based architectures cannot look back and
readjust the weight of previous information. For
better performance, desired information should be
after the question. Example of ChatGPT prompt to
RTE:
Having premise <here is a premise> judge if the
following hypothesis <here is a hypothesis> are
logically connected with the premise? Answer "en-
tailment" if yes, or "not_entailment" if no.
RWKV prompt taking into account the characteris-
tics of the RNN:
Can you tell me if the hypothesis is entailment or is
not entailment to the premise?

Task Name Measure

type

ChatGPT
[%]

RWKV-4 SOTA
[%]

adapted [%]

Aggression F1 Macro
Accuracy
MathQA
F1 Macro
Sarcasm
F1 Macro
TweetSent
F1 Macro
Unhealthy

69.10
71.40
49.88
63.32
45.21

56.66
80.69
50.96
52.50
43.30

74.45
83.20
53.57
72.07
50.96

Table 7: ChatGPT and RWKV-4-Raven-14B perfor-
mance comparison in Aggresion (Wulczyn et al., 2017),
Sarcasm (Siddiqui, 2019), Unhealthy (Price et al.,
2020), MathQA (Cobbe et al., 2021), and TweetSent
(Barbieri et al., 2020) benchmarks. SOTA is provided
as a supplementary reference.

premise: <here is a premise>
hypothesis: <here is a hypothesis>

While separating the instruction from the input
is relatively easy to do, other aspects of prompt
engineering are harder to quantify. Testing the ap-
proach of stating the input after the question on
multiple other tasks, shown in tab. 7, suggests that
better prompts might reduce the disparity between
models. Raven achieves comparable result to Chat-
GPT on unhealthy conversation detection and even
surpasses it on the sarcasm detection dataset. While
this approach in prompting looks necessary, it alone
is not enough to replace the capability of having
free access to the whole context. Therefore, prompt
engineering seems to be of signiﬁcantly more im-
portance to the RNN models compared to stan-
dard transformers. It is entirely possible that good
prompts to RNN models do not mean additional
restrictions, but should simply be constructed using
completely different guidelines. While authors of a
forementioned paper (Koco´n et al., 2023) perform
chain-of-thought to improve results on MathQA
dataset, trying the same approach on the Raven
model achieved a very low accuracy of 48.11%.
Despite that, using a very similar prompt without
the chain-of-thought outperformed ChatGPT on
this dataset. It would not be surprising if, thanks
to the hidden state, RNN models do not need addi-
tional steps during inference, and adding those only
makes the model forget the information it analyzed
and deemed important. This further emphasizes the
importance of the order of information the model
receives.
The template used to prompt the Raven model in
MathQA with chain-of-thought prompting:
Write the reasoning and highlight the answer to
the question at the end in format: ’Answer: ’. The
question is: <here is question>

The template used to prompt the Raven model in
MathQA without chain-of-thought prompting:
Write the answer to the math question in the format:
’Answer: ’.
The question is: <here is question>

K Cases

In this part, we present a few instances of outputs produced by RWKV model using a Chat interface56.

RWKV-4-Raven-14B

Tell me about ravens.

Ravens are large, black birds with a distinctive white head and neck. They are found in most parts
of the world, including North America, Europe, Asia, and Australia. Ravens are known for their
intelligence and problem-solving abilities. They are also considered to be symbols of death and
transformation in many cultures. Ravens are often associated with the afterlife or death because
they have been known to eat carrion or even other birds. In some cultures, ravens are also believed
to be messengers of the gods or guardians of treasure.

RWKV-4-Raven-14B

Explain the concept of machine learning to a 10-year-old.

Machine learning is like a super-smart computer that can learn on its own. It can recognize patterns
and make decisions based on what it has learned. Just like how you might remember something
from the past, the computer can remember what it’s learned and use it to make predictions or
decisions in the future. It’s like having a smart friend who always knows what to do!

RWKV-4-Raven-14B

Write a short poem describing nature’s beauty in springtime.

Nature’s beauty in springtime is rare,
A symphony of colors, a gentle surprise.
The ﬂowers bloom, the birds sing and sing,
As the sun shines down, warming our skin.
The trees sway in the gentle breeze,
As we take in their beauty with ease.
The sky is blue, the clouds are white,
A sight to behold, it’s truly divine.

5https://github.com/BlinkDL/ChatRWKV
6https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio

RWKV-4-Raven-14B

Below is the numerical stable formula for updating the state variable in RWKV:

t = ep−qat−1 + eu+kt−qvt
a∗
t = ep−qbt−1 + eu+kt−q
b∗

wkvt =

a∗
t
b∗
t

Please write out the corresponding pytorch code to do so, where both at and bt are
torch.Tensor([n])

Here is the PyTorch code to update the state variable in RWKV using the given formula:

import
d e f u p d a t e _ s t a t e ( a , b , u , v , p , q ) :

t o r c h

a _ t = t o r c h . exp ( p − q ) * a + t o r c h . exp ( u + k − q ) * v
b _ t = t o r c h . exp ( p − q ) * b + t o r c h . exp ( u + k − q )
r e t u r n a _ t , b _ t
a , b = t o r c h . r a n d n ( 2 , 3 )
u = t o r c h . r a n d n ( 2 )
v = t o r c h . r a n d n ( 2 )
wkv = t o r c h . r a n d n ( 2 )
a _ t , b _ t = u p d a t e _ s t a t e ( a , b , u , v , wkv ,
p r i n t ( " a _ t : " , a _ t )
p r i n t ( " b _ t : " , b _ t )

t o r c h . r a n d n ( 2 ) )

This code deﬁnes a function update_state that takes in the state variables a and b, the control
variables u and v, and the parameters wkv and torch.randn. It then uses the PyTorch torch.exp
function to compute the exponential of the control variables and the log of the normalization factor.
Finally, it uses these values to update the state.

RWKV-4-Raven-14B

Explain the following metaphor: Life is like cats.

The metaphor "Life is like cats" suggests that life can be unpredictable and full of unexpected
twists and turns, just like a cat’s behavior. It implies that we cannot fully control or predict the
outcome of our lives, just as we cannot control the movements of a cat. The metaphor also implies
that life is complex and multifaceted, with many different variables and factors at play. Just as a
cat can have multiple personalities or moods, life can have many twists and turns that are difﬁcult
to predict or understand. Overall, the metaphor suggests that life is complex and unpredictable,
much like a cat’s behavior.

RWKV-4-Raven-14B
Test the convergence of (cid:80)∞

i=0 2−i.

The given series is a geometric series with the ﬁrst term a0 = 2 and common ratio r = 1
ﬁnd the sum of this series using the formula for a geometric series:

2 . We can

∞
(cid:88)

i=0

ai =

a0
1 − r

=

1
2
1 − 1
2

=

1
1 − 1
2

=

2
1

= 2

So, the sum of the given series is 2 .

RWKV-4-Raven-14B

I would like to cook some Chinese food at home. Do you have any suggestions on the meal I
could choose?

Yes, of course! If you are not very experienced in cooking Chinese food, you could start with
making noodles or rice, then make a dish using one of those. I also think making stir-fry with
some meat is an excellent idea.

I want to make something more special. What about red braised pork belly?

Yes, red braised pork belly is also very popular and tasty.

Tell me the steps to cook.

You will need a few ingredients such as pork belly, star anise, sugar, ginger, scallions and wine.
Put all of these in a pot, cook on medium heat and simmer until the meat is tender. After that,
remove the meat from the pot and reserve the sauce. Slice the meat and add some ginger slices
into it, then boil until it’s almost done.

What do I need to be careful with the most during the cooking process?

To cook pork belly properly, it is very important to monitor the heat of the stove and be patient
with the cooking process. Overcooking it can cause the pork to be tough, dry and difﬁcult to chew.
You can add a few ingredients like soy sauce or black beans to give the pork more ﬂavor.

Tell me more about the proper heat and timing.

If the temperature of the water is too high, it can damage the collagen and connective tissues,
resulting in tough pork belly.
It is essential to control the temperature carefully, especially
when braising the pork. Additionally, keep the water simmering but not boiling to maintain a
low simmer, as high heat will dry out the pork belly and result in tough and unpalatable meat.
Lastly, monitor the heat during cooking and make sure the heat is medium, as this is what is most
appropriate for slow cooking pork belly.

I see. Thank you for your advice.

You’re welcome! It was a pleasure chatting with you. Let me know if you need any further
assistance in the future.

RWKV-4-Raven-14B

If all apples are fruits and all fruits are sweet, are all apples sweet? Please explain your
answer in detail.

Yes, all apples are fruits, and all fruits are sweet. Therefore, it is safe to assume that all apples are
sweet.

If birds can ﬂy. Penguins are birds. Can penguins ﬂy? Please explain your answer in detail.

No, penguins cannot ﬂy. Penguins are birds, but they are not capable of ﬂight like other birds.
They have wings and can glide for short distances, but they cannot take off or ﬂy long distances
like other birds.

The day before two days after the day before tomorrow is Saturday. What day is it today?
Please explain your answer in detail.

Today is Friday. Two days after tomorrow is Sunday. Tomorrow is Saturday.

If ﬁve cats can catch ﬁve mice in ﬁve minutes, how long will it take one cat to catch one
mouse? Please explain your answer in detail.

If ﬁve cats can catch ﬁve mice in ﬁve minutes, it means that each cat can catch one mouse in one
minute. To ﬁnd out how long it will take one cat to catch one mouse, we can use the concept of
proportionality.
Let’s assume that it takes x minutes for a cat to catch a mouse. Then, the number of mice caught
by a cat in one minute is equal to the number of cats multiplied by the number of mice caught by
each cat: x = 5 * 1 = 5
So, it will take 5 minutes for one cat to catch one mouse.

RWKV-4-Raven-14B

Summarize the following paragraph.

<The Abstract of this paper goes here>

The paper presents a novel architecture, Receptance Weighted Key Value (RWKV), that combines
the efﬁcient parallelizable training of Transformers with the efﬁcient inference of RNNs. The
model is able to scale to tens of billions of parameters and exhibits linear computational complexity
during training and inference, making it a promising alternative to Transformers for sequence
processing tasks.

Please further summarize it in one sentence.

The researchers have developed a new architecture that combines the efﬁcient parallelizable
training of Transformers with the efﬁcient inference of RNNs, which allows for linear scaling in
memory and computational requirements.


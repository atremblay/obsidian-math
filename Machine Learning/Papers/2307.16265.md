---
title: "Recent Advances in Hierarchical Multi-label Text Classification: A Survey"
authors:
    - Rundong Liu 
    - Wenhan Liang
    - Weijun Luo
    - Yuxiang Song
    - He Zhang
    - Ruohua Xu
    - Yunfeng Li
    - Ming Liu
published: "2023-07-30"
url:
    - "[paper](https://arxiv.org/pdf/2307.16265.pdf)"
    - "[arXiv](https://arxiv.org/abs/2307.16265)"
---

# Recent Advances in Hierarchical Multi-label Text Classification: A Survey
###### Authors
<ul>
<li class="author">Rundong Liu</li>
<li class="separator author">|</li>
<li class="author">Wenhan Liang</li>
<li class="separator author">|</li>
<li class="author">Weijun Luo</li>
<li class="separator author">|</li>
<li class="author">Yuxiang Song</li>
<li class="separator author">|</li>
<li class="author">He Zhang</li>
<li class="separator author">|</li>
<li class="author">Ruohua Xu</li>
<li class="separator author">|</li>
<li class="author">Yunfeng Li</li>
<li class="separator author">|</li>
<li class="author">Ming Liu</li>
</ul>

```dataview
table without ID url from "Mathematics/Machine Learning/Papers/2307.16265.md"
```


#### Abstract

Hierarchical multi-label text classification aims to classify the input text into multiple labels, among which the labels are structured and hierarchical. It is a vital task in many real world applications, e.g. scientific literature archiving. In this paper, we survey the recent progress of hierarchical multi-label text classification, including the open sourced data sets, the main methods, evaluation metrics, learning strategies and the current challenges. A few future research directions are also listed for community to further improve this field.


Keywords: Hierarchical multi-label · Text classification · Label imbalance $\cdot$ Label correlation

## 1. Introduction

The recent advances in deep learning, large language modeling, instruction learning and human-AI alignment has revolutionized the NLP community. Among all these different kinds of NLP tasks, text classification is still regarded as the backbone for most downstream NLP applications. In the real world scenario, most text classification tasks are multi-label ones. For example, the hierarchical categorization of scientific literature in arXiv, the grouping of products in online shopping sites, as well as the management of papers in e-libraries. Conceptually, hierarchical multi-label text classification means that text are associated with multiple labels, for which the labels are organized in a structured hierarchy. Mathmatically, hierarchical multi-label classification can be defined as a function: $f: X \rightarrow Y^{*}$, where $X$ is the sample space and $Y^{*}$ is the power set of all possible label sets. Each element in $Y^{*}$ is an ordered tag sequence $\left(y_{1}, y_{2}, \ldots, y_{k}\right)$, representing the hierarchical path from the root node to the leaf node. The goal of hierarchical multi-label classification is to learn a function $f$, so that for any given sample $x \in X, f(x)$ can output one or more labels which are most relevant to $x$. Traditionally, the one-vs-rest or one-vs-one learning strategy is a common way to learn hierarchical multi-label classifiers, but it ignores the information of the label hierarchy and the learning process is expensive. Modern approaches utilize a deep learning encoder based on a Binary Cross Entropy loss and do the training in a end-to-end manner. In addition, the label information can also be leveraged in the learning process.

Currently, there are more advanced models and learning approaches for hierarchical multi label tasks, but few literature has summarized these methods. Therefore, we conduct a survey of recent advances on hierarchical multi-label text classification. This survey is organized in the following way: Section 2 summarizes the data sets used for hierarchical multi-label classification. Section 3 mainly discusses the models and methods used for hierarchical multi-label text classification, which are mainly divided into four major categories. In section 4, we discuss some machine learning strategies for hierarchical multi-label text classification. Subsequently, section 5 show the widely used evaluation metrics and section 6 will mention some existing challenges and the last section is the conclusion and some suggestion for future work.

## 2. Hierarchical Multi-label Classification Data sets

In this section, we will introduce the common hierarchical multi-label classification data sets. In total, 13 data sets are included and Table 1 shows the text domain, number of labels and label depth of each hierarchical multi-label data set.

Table 1. A summary of hierarchical multi-label data sets

| Dataset | Domain | Number of labels | Label depth |
| :---: | :---: | :---: | :---: |
| YELP | dining reviews | 539 | - |
| Amazon 3M | product reviews | $2,812,281$ | - |
| IMBD-Multi | movie reviews | 81,742 | 21 |
| WOS | academic articles | - | - |
| Arxiv | academic issues | - | - |
| Pubmed | biomedical articles | 17693 | 15 |
| Reuters | news | - | - |
| NYTimes | news | 2318 | 10 |
| HUPC | patent | - | - |
| BGC-EN | books | 146 | 4 |
| WIPO-alpha | patent | 5229 | 4 |
| DBpedia | wikipedia | - | - |
| SciHTC | scientific articles | 1233 | - |

The YELP data set [16] is a famous sentimental data set,which sourced from the US restaurant review website. User reviews of YELP-2 mainly use positive and negative emotion labels, while YELP-5 reviews have five different scores. Amazon Product Reviews 31] contains a large number of user product reviews of Amazon, with about 8 million reviews. IMDB-Multi [16] has the meta data for 81,742 movies, each movie can have different genres. The SciHTC[^1] contains 186,160 papers and 1,233 categories from the ACM CCS tree. The WOS data set[^2] contains web of science articles, with 134 catalogues and 46,985 instances, covering all aspects. Pubmed[^3] is from the biomedical domain and researcher can retrieve the latest literature from the official NIH website. HUPC[^4] and WIPO[^5] are patent data sets,including patents'ID, authors,classes. Reuters data set has two versions: RCV1[^6] is modified on the Reuters-21578 data set and RCV2 has nearly 5 million news in 13 different languages. NewYork Times data set[^7] contains 1.8 million articles from New York Times during 1987-2007. Blurb Genre Collection (BGC)[^8] is a data set of book advertising descriptions and it consists of 91,892 instances, including the book's title, the author, url, ISBN, etc. DBpedia[^9] is extracted from Wikipedia. It is updated monthly and the label scheme is adjusted as well.

[^1]: https://arxiv.org/pdf/2211.02810v1.pdf
[^2]: https://arxiv.org/pdf/1709.08267v1.pdf
[^3]: https://pubmed.ncbi.nlm.nih.gov/download/
[^4]: https://arxiv.org/pdf/2207.04043.pdf
[^5]: https://www.wipo.int/classifications/ipc/en/ITsupport/Categorization/dataset/index.html
[^6]: https://dl.acm.org/doi/pdf/10.5555/1005332.1005345
[^7]: https://catalog.ldc.upenn.edu/LDC2008T19
[^8]: https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/blurb-genre-collection.html
[^9]: https://www.researchgate.net/publication/259828897_DBpedia_-_A_Largescale_Multilingual_Knowledge_Base_Extracted_from_Wikipedia 

## 3. Main Approaches for hierarchical multi-label classification

In this section, we consider four main approaches for hierarchical multi-label text classification: tree-based approach, embedding based approach, graph based approach and the ensemble approach.

### 3.1. Tree-based approach

This method is based on probabilistic label tree, which was originally developed for extreme multi-label classification, where a probabilistic label tree (PLT) is used to partition labels, where each leaf in PLT corresponds to an original label and each internal node corresponds to a pseudo-label (meta-label). Then by maximizing a lower bound approximation of the log likelihood, each linear binary classifier for a tree node can be trained independently with only a small number of relevant samples. Parabel [17] is a traditional label tree-based method using bag-of-words (BOW) features. FASTTEXT [11] and EXTREMETEXT 27] extended Parabel by using dense features. AttentionXML [30], which used attention model and a PLT to further improve the classification performance. FastXML [18] draws on thee multi-label random forest (MLRF) algorithm and sublinear ranking label (LPSR) partition algorithm. Bonsai 12 proposed to build a shallow and diverse label tree structure, in which all the labels are segmented into $\mathrm{K}$ sets through the clustering, but it needs high space complexity because of using linear classifiers. HPT [24] make efforts to transform hierarchy text classfication(HTC) into a hierarchy-aware multi-label mask language model(MLM) problem that focuses on bridging two gaps between HTC and MLM.

For the tree-based methods, their advantages are that they can use the hierarchical structure between labels more accurately and the prediction results of the model are easy to explain. The disadvantage is that there is a high requirement for label level knowledge that needs to be acquired in advance and accurate hierarchy information is required. Besides, because it uses a relatively simple tree structure model, it is difficult to deal with the complex dependency relationship between labels, which needs to be represented by a more powerful model.

### 3.2. Embedding based approach

Embedding based methods arise with deep learning models, most of these methods, use an encoder to represent the input text and learn a mapping function for input text and the labels. HyperIM [6] embeds both document words and labels jointly in the hyperbolic space to preserve their latent structures. The innovation of this method is that it can connect words in documents and labels, and use the most relevant part of documents to build the representations of the article, so as to improve the accuracy of classification. HTrans [2], a hierarchical transfer learning approach uses a Gated Recurrent Unit based architecture coupled with attention mechanism. In Hierarchical Fine-Tuning (HFT) [20] the parameters of the word embedding layer and the convolution layer are mainly transferred from top to bottom, that is, the parameters of the upper layer structure are used to learn the labels of the lower layer. At the same time, in order to avoid adverse effects caused by excessive correlation gaps between distant levels, parameter fine-tuning is only performed between adjacent levels. C-HMCNN [8] uses a constraint loss function to make the hierarchical structure of the lower layer to predict the upper layer. LA-HCN [33] uses the label-based attention to bridge the labels and document content. Unlike the features extracted from typical attention may be diluted, it can extract meaningful information corresponding to different labels to learn learning disjoint features for each hierarchy. The HMCN 25] simultaneously optimizes local and global loss functions for discovering local hierarchical class-relationships and global information from the entire class hierarchy while penalizing hierarchical violations. The embedding based approach often needs a large amount of training examples, it relies on the attention mechanism and regularization from the label structure. A weakness is that there is no explainability for wrongly predicted labels.

### 3.3. Graph based approach

Graph based approaches regard the whole document and hierarchical labels as a single graph, where the node type may include token, label, sentence, topic or even some meta information. A graph neural network is then applied on the graph and the hierarchical multi-label classification task can be converted into a node classification task. For example, heterogeneous graphs are used in 29] to model the labels' hierarchy, their statistical dependencies and meta data. Loosely coupled graph convolutional neural network (LCGCN) [28] mainly solves the over-smoothing problem caused by too many nodes or edges of the GCN graph model. There are two parts in it, one is the core graph, which is used to extract word embeddings and label embeddings, the other one is the document-word graph, which is mainly used to capture the relationship between documents and words. In another hierarchy-aware global model [35], the label hierarchy is represented as a directed graph with two sub models: HiAGM-LA (the label features can be fused with each other, and the text and label are independently expressed and predicted by attention mechanism) and HiAGM-TP (text features can be directly input into the structural encoder to participate in the feature calculation). This model captures the hierarchical information and fully utilizes the interaction of text features and label features.

The graph based approach inputs the hierarchical information of labels into the network as known knowledge, and some documents also use the meta data of documents as input. The advantage of this method is that it can better handle the complex dependency between tags. Besides, the model design structure is not complex, which only needs to model and represent the hierarchical structure. However, the classifier cannot make more targeted use of the hierarchical structure between labels. Similar with the embedding approach, the prediction results of the model are difficult to explain.

### 3.4. Ensemble approach

Ensemble is widely used in statistical machine learning. In terms of hierarchical multi-label classification, several models can be trained for each of the levels of the labels, which can be conducted in parallel. In the prediction time, a voting mechanism can be conducted, followed by further cross checking among the predicted labels. However, this method is mostly used in the industrial field and is suitable for large engineering projects.

Among different ensemble approaches, one-vs-one [7] and one-vs-all [34] are the two typical ways. One-vs-one refers to learning a classifier between each two categories. It works well when the total amount of labels is small, but as the number of levels of labels increases, the number of classifiers also increases dramatically. When there are $\mathrm{n}$ categories here, then the number of the classifiers will grow to $C_{n}^{2}$. In contrast, one-vs-all means that a single classifier is trained for each label, and the classifier treats its corresponding category as positive examples and treats the remaining categories as negative examples. In the shared task 5 track 1 of NLPCC2022 multi-label classification for scientific literature 13, the task requires participants to build multi-label text classification models for scientific abstracts from the chemistry domain. Among all the submitted system, the best system [22] was obtained by an ensemble model with more than five different deep learning models. Another boosting ensemble method is derived from Devil's advocate [10]. This method requires at least three models to be integrated, and a new loss which uses the idea of generated confrontation to make the target model finally converge is designed.

## 4. Machine Learning Strategies

Many machine learning strategies were adopted for hierarchical multi-label classification. These learning strategies usually consider multiple aspects, such as data, label and training. We list the following machine learning strategies.

###### Self-training 
Self training is a semi supervised learning method in which a small amount of labeled data is used for initial training, and then prediction results are used to generate more training data. In hierarchical multi label classification tasks, self training can generate more training data by using the prediction results of the current model, and then use these data for the next round of training.

###### Weakly-supervised learning 
In reality, a real situation is that there is no big amount of human labeled documents to train classifiers, so approaches based on only class surface names as supervision signals are explored. An example is TaxoClass [19]. First, a pre-trained textual entailment model is used to calculate the document-class similarity. After that, the calculated similarity is used to determine the core class of the article. Then training data extracted from document core classes is used to train a text classifier. The classifier includes a document encoder based on pre-trained BERT, a class encoder capturing class taxonomy structure, and a text matching network computing the probability of a document being tagged with each class. The class Encoder is a graph neural network to capture the hierarchical information of label. This method can train a large number of unlabeled documents, but because of the unlabeled data, the determination of the core class of the document may not be so accurate, which may affect the final accuracy. ASR2 [21] is proposed as a general, model-agnostic weakly supervised leading framework, which is dedicated to alleviate the data imbalance issue. It calculates a probabilistic margin score based on the output of the current model to measure and rank the cleanliness of each data point. Then, the ranked data is sampled based on both class-wise and rule-aware ranking. Another example [15] is the use of constructed pseudo-documents to address label sparsity by improving the dataset in three different ways. Assuming that words and documents are located in the same semantic space, a spherical distribution is established to sample keywords that generate pseudo-documents, thereby improving the generalization of seed information.

###### Data augmentation 
Data augmentation is a commonly used training strategy that can increase the diversity of training data and help models better generalize to new data. In hierarchical multi label classification tasks, data augmentation can be used to generate more high-quality training data, and can help models learn hierarchical relationships between labels by generating data related to hierarchical relationships. Data enhancement can be implemented in the following ways: Synonym Replacement, Random Insertion, Random Deletion, Random Swap, and Sentence Rearrangement. The above methods can be randomly applied to training data with a certain probability, thereby expanding the data set and increasing the generalization ability and robustness of the model. At the same time, it is also important to note that excessive data enhancement may lead to over fitting of the model, so it is necessary to make choices based on specific tasks and data characteristics.

###### Label enhancement
Label enhancement is a method of enhancing original label information through the use of external resources (such as knowledge maps, dictionaries, corpora, etc.). In hierarchical multi label classification tasks, label enhancement can help models more accurately understand and depict the relationship and semantic information between different labels, thereby improving the classification accuracy and generalization ability of the model. However, it should be noted that excessive label enhancement may introduce noise and incorrect labels, thereby reducing the performance and generalization ability of the model. Therefore, this method needs to be used appropriately. MHG [29] modeles metadata and its topological relationships to construct a metadata heterogeneous graph,to capture label dependencies. HVHMC 28] integrates hierarchical information through graph convolutional network learning as a way to capture the horizontal and vertical dependencies between labels.

###### Post-processing 
In hierarchical multi-label text classification, post processing refers to a strategy of adjusting the relationship between labels when the model has already predicted multiple labels. It is usually used to solve situations where there is inconsistency or conflict between the labels predicted by the model. Common post processing strategies include label filtering (remove predicted untrustworthy or irrelevant labels by setting thresholds or other rules), label combination (combine the predicted labels into higher-level labels to reflect their relationship), label conflict (post-processing predicted labels to resolve conflicts or inconsistencies between them). The post-processing steps may vary according to the hierarchy of the label scheme.

## 5. Evaluation Metrics for Hierarchical Multi-label Classification.

For a hierarchical multi label classification task, three types of metrics are often used: label-based, example-based and ranking based. Precision, recall, and F1-score are often used as label-based evaluation metrics. Example-based evaluation metrics includes accuracy, hamming loss, subset accuracy and information contrast model [1]. Ranking-based evaluation metrics includes Precision@k and Mean Reciprocal Rank. Precision@k and mean reciprocal rank are two widely used evaluation metrics for assessing the performance of machine learning classifiers and ranking tasks, respectively.

## 6. Challenges

Even though many methods have been proposed, there are still many challenges for hierarchical multi-label text classification, including but not limited to the label sparsity and imbalance, low resources labeling data, low accuracy in deep levels of labels, and the extreme multi-label problem.

Label sparsity and imbalance Label sparsity and imbalance refers to the long-tail distribution of the labels, by which only a small set of labels have many training examples and a large number of the labels are low frequent ones. The model can easily over-fit the high-frequency labels and under-fit the low frequency ones. Often times, when a label appears only a few times, the classifier may ignore it and the general performance is not affected. The model will be more reliable to the data sets with relatively balanced distribution of labels. Some models try to alleviate the imbalance of labels at this level by utilizing predicted labels from the previous level or leveraging global information, such as HMCN-F [26], HTF-CNN 20. However, due to the use of a large number of parameters and the lack of overall information, these methods are prone to exposing bias issues. HiAGM [35], improved with a bidirectional computing structure encoder, enhances its ability to handle sparse data. HiLAP [14] adopts reinforcement learning to to learn a Label Assignment Policy. ARS2 [21] is a framework which proposed two adaptive sampling strategies to address data imbalance issues. Meta-LMTC [23] is used to solve scenes with few and zero shots. Using a meta-learning approach with fine-tuning of parameters to quickly make labels with fewer instances adaptable to the task. And the balancing loss functions for multi-label text classification [9] is also introduced to deal with long-tailed distributions.

Low resource labeled data The lack of labeled data another challenge. Zeroshot learning is a common setting, which refers that some labels do not have corresponding training data, only descriptions of the labels. A normal method is to convert it into a nearest neighbor search problem. Currently, one model [5] based on LWAN by using the label hierarchy is aimed to improve zero-shot learning. An end-to-end structural contrasting representation learning approach 32 is proposed by raising a novel randomized text segmentation method. However, the enormous label space and complex association between labels and text lead to extensive calculations and low accuracy.

Lower accuracy in deeper levels In most hierarchical multi-label classification tasks, deeper level labels involve more specific concepts or categories, and these categories may appear less frequently in the data set, which makes it difficult for the model to extract deep features. Besides, the incorrect prediction at one level may affect the results of its child labels. The error prediction can be propagated to the deeper levels, which causes low accuracy in deep level label prediction.

Extreme multiple label problem The extreme multi-label text classification is the problem of labeling an instance with a small subset of relevant labels chosen from an extremely large pool of possible labels. Due to the lack of enough training data, clustering and ranking algorithms are often leveraged before the classification step, such as Divide and Conquer [3] and Seq2Set [4].

## 7. Conclusions and Future Work

In this paper, we conduct a survey for hierarchical multi-label text classification, including the data sets, main approaches, learning strategies, evaluation metrics and challenges. We find that early hierarchical multi label classification models were based on a tree structure, and more recent approaches rely on deep learning models, especially Transformer and BERT. Currently, the graph neural network based approach is the main stream, where the hierarchical label information is modeled together with the input text in a graph and a deep graph neural network is learned. The current state of art uses an ensemble approach, where a set of classifiers based on the hierarchical structure of labels are learned, and a global classifier is then developed to predict all the labels. The ensemble approach is quite effective but few formal results are published in academia. We consider a few future research directions: i) Leverage instruction learning for hierarchical multi-label text classification, as prompt learning has shown quite strong performance. Designing appropriate prompt for hierarchical multi-label classifiers can be regarded as a weakly supervised learning problem. ii) Deal with the label imbalance and sparsity problem. iii) Inject domain knowledge into the classifier, this is often efficient when labeled data is scare and domain knowledge is strait-forward. iv) As recent big language models like ChatGPT and GPT4 are released, the ability of zero-shot learning of these language models remains as question. v) Incremental hierarchical multi-label learning, the label hierarchy can evolve with more labels and different hierarchy, continual learning of the classifier is a key problem in the real world scenario.

## 8. References

1. Enrique Amigo and Agustín Delgado. Evaluating extreme hierarchical multi-label classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5809-5819, Dublin, Ireland, May 2022. Association for Computational Linguistics.
2. Siddhartha Banerjee, Cem Akkaya, Francisco Perez-Sorrosal, and Kostas Tsioutsiouliklis. Hierarchical transfer learning for multi-label text classification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6295-6300, Florence, Italy, July 2019. Association for Computational Linguistics.
3. Jose Barros, Matias Rojas, Jocelyn Dunstan, and Andres Abeliuk. Divide and conquer: An extreme multi-label classification approach for coding diseases and procedures in Spanish. In Proceedings of the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI), pages 138-147, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics.
4. Jie Cao and Yin Zhang. OTSeq2Set: An optimal transport enhanced sequence-toset model for extreme multi-label text classification. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 55885597, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
5. Ilias Chalkidis, Manos Fergadiotis, Sotiris Kotitsas, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. An empirical study on large-scale multilabel text classification including few and zero-shot labels. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7503-7515, Online, November 2020. Association for Computational Linguistics.
6. Boli Chen, Xin Huang, Lin Xiao, Zixin Cai, and Liping Jing. Hyperbolic interaction model for hierarchical multi-label classification. CoRR, abs/1905.10802, 2019.
7. Suthipong Daengduang and Peerapon Vateekul. Applying one-versus-one svms to classify multi-label data with large labels using spark. In 2017 9th International Conference on Knowledge and Smart Technology (KST), pages 72-77. IEEE, 2017.
8. Eleonora Giunchiglia and Thomas Lukasiewicz. Coherent Hierarchical Multi-Label Classification Networks. arXiv e-prints, page arXiv:2010.10151, October 2020.
9. Yi Huang, Buse Giledereli, Abdullatif Köksal, Arzucan Özgür, and Elif Ozkirimli. Balancing methods for multi-label text classification with long-tailed class distribution. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8153-8161, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
10. Hwiyeol Jo, Jaeseo Lim, and Byoung-Tak Zhang. Devil's advocate: Novel boosting ensemble method from psychological findings for text classification. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2168-2174, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. 11. Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759, 2016.
11. 
12. Sujay Khandagale, Han Xiao, and Rohit Babbar. Bonsai - Diverse and Shallow Trees for Extreme Multi-label Classification. arXiv e-prints, page arXiv:1904.08249, April 2019.
13. Ming Liu, He Zhang, Yangjie Tian, Tianrui Zong, Borui Cai, Ruohua Xu, and Yunfeng Li. Overview of nlpcc2022 shared task 5 track 1: Multi-label classification for scientific literature. In Natural Language Processing and Chinese Computing: 11th CCF International Conference, NLPCC 2022, Guilin, China, September 2425, 2022, Proceedings, Part II, pages 320-327. Springer, 2022.
14. Yuning Mao, Jingjing Tian, Jiawei Han, and Xiang Ren. Hierarchical text classification with reinforced label assignment. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 2019.
15. Yu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han. Weakly-supervised neural text classification. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management. ACM, oct 2018.
16. Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad, Meysam Chenaghlu, and Jianfeng Gao. Deep learning-based text classification: A comprehensive review. ACM Comput. Surv., 54(3), apr 2021.
17. Yashoteja Prabhu, Anil Kag, Shrutendra Harsola, Rahul Agrawal, and Manik Varma. Parabel: Partitioned label trees for extreme classification with application to dynamic search advertising. In Proceedings of the 2018 World Wide Web Conference, pages 993-1002, 2018.
18. Yashoteja Prabhu and Manik Varma. Fastxml: A fast, accurate and stable treeclassifier for extreme multi-label learning. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 263-272, 2014.
19. Jiaming Shen, Wenda Qiu, Yu Meng, Jingbo Shang, Xiang Ren, and Jiawei Han. TaxoClass: Hierarchical multi-label text classification using only class names. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 42394249, Online, June 2021. Association for Computational Linguistics.
20. Kazuya Shimura, Jiyi Li, and Fumiyo Fukumoto. HFT-CNN: Learning hierarchical category structure for multi-label short text categorization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 811816, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.
21. Linxin Song, Jieyu Zhang, Tianxiang Yang, and Masayuki Goto. Adaptive rankingbased sample selection for weakly supervised class-imbalanced text classification. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1641-1655, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
22. Bo Wang, Yi-Fan Lu, Xiaochi Wei, Xiao Liu, Ge Shi, Changsen Yuan, Heyan huang, Chong Feng, and Xianling Mao. Bit-wow at nlpcc-2022 task5 track1: Hierarchical multi-label classification via label-aware graph convolutional network. In Natural Language Processing and Chinese Computing: 11th CCF International Conference, NLPCC 2022, Guilin, China, September 24-25, 2022, Proceedings, Part II, pages 192-203. Springer, 2022. 23. Ran Wang, Xi'ao Su, Siyu Long, Xinyu Dai, Shujian Huang, and Jiajun Chen. Meta-LMTC: Meta-learning for large-scale multi-label text classification. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8633-8646, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
23. Zihan Wang, Peiyi Wang, Tianyu Liu, Binghuai Lin, Yunbo Cao, Zhifang Sui, and Houfeng Wang. HPT: Hierarchy-aware prompt tuning for hierarchical text classification. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3740-3751, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
24. Jonatas Wehrmann, Ricardo Cerri, and Rodrigo Barros. Hierarchical multi-label classification networks. In International conference on machine learning, pages 5075-5084. PMLR, 2018.
25. Jônatas Wehrmann, Ricardo Cerri, and Rodrigo Barros. Hierarchical multi-label classification networks. 092019.
26. Marek Wydmuch, Kalina Jasinska, Mikhail Kuznetsov, Róbert Busa-Fekete, and Krzysztof Dembczynski. A no-regret generalization of hierarchical softmax to extreme multi-label classification. Advances in neural information processing systems, 31, 2018.
27. Linli Xu, Sijie Teng, Ruoyu Zhao, Junliang Guo, Chi Xiao, Deqiang Jiang, and Bo Ren. Hierarchical multi-label text classification with horizontal and vertical category correlations. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2459-2468, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
28. Chenchen Ye, Linhai Zhang, Yulan He, Deyu Zhou, and Jie Wu. Beyond text: Incorporating metadata and label structure for multi-label document classification using heterogeneous graphs. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3162-3171, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
29. Ronghui You, Zihan Zhang, Ziye Wang, Suyang Dai, Hiroshi Mamitsuka, and Shanfeng Zhu. Attentionxml: Label tree-based attention-aware deep model for high-performance extreme multi-label text classification. Advances in Neural Information Processing Systems, 32, 2019.
30. Jiong Zhang, Wei-Cheng Chang, Hsiang-Fu Yu, and Inderjit Dhillon. Fast multiresolution transformer fine-tuning for extreme multi-label text classification. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 7267-7280. Curran Associates, Inc., 2021.
31. Tianyi Zhang, Zhaozhuo Xu, Tharun Medini, and Anshumali Shrivastava. Structural contrastive representation learning for zero-shot multi-label text classification. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4937-4947, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
32. Xinyi Zhang, Jiahao Xu, Charlie Soh, and Lihui Chen. LA-HCN: Label-based Attention for Hierarchical Multi-label TextClassification Neural Network. arXiv e-prints, page arXiv:2009.10938, September 2020.
33. Xiuhao Zhao, Zhao Li, Xianming Zhang, Jibin Wang, Tong Chen, Zhengyu Ju, Canjun Wang, Chao Zhang, and Yiming Zhan. An interactive fusion model for hierarchical multi-label text classification. In Wei Lu, Shujian Huang, Yu Hong, and Xiabing Zhou, editors, Natural Language Processing and Chinese Computing, pages 168-178, Cham, 2022. Springer Nature Switzerland.
34. Jie Zhou, Chunping Ma, Dingkun Long, Guangwei Xu, Ning Ding, Haoyu Zhang, Pengjun Xie, and Gongshen Liu. Hierarchy-aware global model for hierarchical text classification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1106-1117, Online, July 2020. Association for Computational Linguistics.

![](https://cdn.mathpix.com/cropped/2023_08_03_29c1a5df5bb0dc3ad331g-14.jpg?height=526&width=1195&top_left_y=2233&top_left_x=-1)


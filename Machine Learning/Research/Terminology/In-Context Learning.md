 

Generally refers to adapting to a task by providing training examples as additional textual input, without performing gradient-based updates. This technique imposes a limit on size of the training dataset due to context length limits. Recent work from [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://aclanthology.org/2021.acl-long.353.pdf) and [What Makes Good In-Context Examples for GPT-3](https://arxiv.org/abs/2101.06804v1) demonstrate that the choice of in-context training examples, and the order in which they are presented have large effects on performance.

From [[A Few More Examples May Be Worth Billions of Parameters]]

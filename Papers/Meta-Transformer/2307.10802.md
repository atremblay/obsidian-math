# Meta-Transformer: A Unified Framework for Multimodal Learning

 Yiyuan Zhang\({}^{1,2}\)1 Kaixiong Gong\({}^{1,2}\)1 Kaipeng Zhang\({}^{2}\)2

**Hongsheng Li\({}^{1}\) Yu Qiao\({}^{2}\) Wanli Ouyang\({}^{2}\) Xiangyu Yue\({}^{1}\)2\({}^{\dagger}\)3**

\({}^{1}\)Multimedia Lab, The Chinese University of Hong Kong \({}^{2}\)Shanghai AI Lab

yiyuanzhang.ai@gmail.com, kaixionggong@gmail.com, xyyue@ie.cuhk.edu.hk

[https://kxgong.github.io/meta_transformer/](https://kxgong.github.io/meta_transformer/)

Equal contributionCorresponding authorsProject leader

Footnote 1: Project leader

###### Abstract

Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities (_e.g._ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a **frozen** encoder to per

Figure 1: **Unified Multimodal Learning**. Meta-Transformer utilizes the same backbone to encode natural language, image, point cloud, audio, video, infrared, hyperspectral, X-ray, time-series, tabular, Inertial Measurement Unit (IMU), and graph data. It reveals the potential of transformer architectures for unified multi-modal intelligence.

form multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modalities with unpaired data. Experiments on different benchmarks reveal that Meta-Transformer can handle a wide range of tasks including fundamental perception (text, image, point cloud, audio, video), practical application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph, tabular, and time-series). Meta-Transformer indicates a promising future for developing unified multimodal intelligence with transformers. Code will be available at [https://github.com/invictus717/MetaTransformer](https://github.com/invictus717/MetaTransformer).

## 1 Introduction

The human brain, which is considered as the inspiration for neural network models, processes information from various sensory inputs, _e.g._ visual, auditory, and tactile signals, simultaneously. Moreover, knowledge from one source can benefit the comprehension of another. However, in deep learning, designing a unified network capable of processing a wide range of data formats is a non-trivial task due to the significant modality gap [1; 2; 3].

Each data modality presents unique data patterns, which makes it difficult to adapt models trained on one modality to another. For instance, images exhibit a high degree of information redundancy due to densely packed pixels, which is not the case with natural language [4]. Point clouds, on the other hand, have a sparse distribution in 3D space, making them more susceptible to noise and challenging to represent [5]. Audio spectrograms are time-varying and non-stationary data patterns consisting of combinations of waves across frequency domains [6]. Video data contains a sequence of image frames, which gives it the unique capability to capture both spatial information and temporal dynamics [7]. Graph data represents entities as nodes and relationships as edges in a graph, modeling complex, many-to-many relationships between entities [8]. Owing to the substantial differences inherent to various data modalities, it is common practice to utilize distinct network architectures to encode each modality separately. For instance, Point Transformer [9] leverages vector-level position attention to extract structural information from 3D coordinates, but it cannot encode an image, a natural language paragraph, or an audio spectrogram slice. Therefore, designing a unified framework capable of utilizing a modality-shared parameter space to encode multiple data modalities remains a significant challenge. Recently, the development of unified frameworks such as VLMO [2], OFA [10], and BEiT-3 [3] have improved the ability of the network for multimodal understanding, through large-scale multimodal pretraining on paired data [3; 10; 2], but they are more focused on vision and language, and unable to share the whole encoder across modalities

The transformer architecture and attention mechanism, proposed by Vaswani _et al._ in 2017 [11] for natural language processing (NLP), have made a significant difference in deep learning [12; 11; 13; 14; 15; 16]. These advancements have been instrumental in enhancing perception across different modalities such as 2D vision (including ViT [17; 18] and Swin Transformer [19]), 3D vision (such as Point Transformer [9] and Point-ViT [20; 21]), and audio signal processing ( AST [6]), _etc._ These works have demonstrated the versatility of transformer-based architectures, inspiring researchers to explore _whether it's possible to develop foundation models capable of unifying multiple modalities, ultimately achieving human-level perception across all modalities_.

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline Method & Modalities & Share Parameters & Unpaired Data \\ \hline Transformer [11] & & ✘ & ✘ \\ \hline ViT [13], Swin Transformer [19], MAE [4] & ✘ & ✘ & ✘ \\ \hline Point Transformer[9], PCT [22], Point ViT [21] & ✘ & ✘ & ✘ \\ \hline AST [6], SSAST [23] & ✘ & ✘ & ✘ \\ \hline CLIP [24], Flamingo [25], VLMO [2], OFA [10] & ✘ & ✘ & ✘ \\ \hline BEiT-3 [3] & ✘ & Several Layers & ✘ \\ \hline ImageBind [26] & ✘ & ✘ & ✘ \\ \hline Meta-Transformer \({}_{\text{(ex)}}\) & ✘ & ✘ & Whole Backbone & ✔ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison between Meta-Transformer and related works on perception tasks.

In this paper, We explore the potential of transformer architecture to process 12 modalities including images, natural language, point cloud, audio spectrogram, video, infrared, hyperspectral, X-Ray, IMU, tabular, graph, and time-series data, as shown in Figure 1. We discuss the learning process with transformers for each modality and address the challenges associated with unifying them into a single framework. Consequently, we propose a novel unified framework named Meta-Transformer for multimodal learning. **Meta-Transformer is the first framework to simultaneously encode data from a dozen of modalities using the same set of parameters**, allowing a more cohesive approach to multimodal learning (as shown in Table 1). Meta-Transformer incorporates three simple and effective components: a modality-specialist (SS 3.2) for data-to-sequence tokenization, a modality-shared encoder (SS 3.3) for extracting representations across modalities, and task-specific heads for downstream tasks. Specifically, Meta-Transformer first transforms multimodal data into token sequences that share a common manifold space. Then, a modality-shared encoder with frozen parameters extracts representations, which are further adapted to individual tasks by updating the parameters of downstream task heads and lightweight tokenizers only. Finally, task-specific and modality-generic representations can be effectively learned by this simple framework.

We conduct extensive experiments on various benchmarks of 12 modalities. By utilizing images of LAION-2B [24] dataset for pretraining exclusively, Meta-Transformer demonstrates remarkable performance in processing data from multiple modalities, achieving consistently superior outcomes over state-of-the-art methodologies in different multimodal learning tasks. More detailed experimental settings can be found in SS D.

In conclusion, our contributions can be summarized as follows:

* For multimodal research, we propose a novel framework, Meta-Transformer, which enables a unified encoder to simultaneously extract representations from multiple modalities with the same set of parameters.
* For multimodal network design, we comprehensively examine the functions of transformer components such as embeddings, tokenization, and encoders in processing various modalities. Meta-Transformer provides valuable insights and sparks a promising new direction in developing a modality-agnostic framework capable of unifying all modalities.
* Experimentally, Meta-Transformer achieves outstanding performance on various datasets regarding 12 modalities, which validates the further potential of Meta-Transformer for unified multimodal learning.

## 2 Related Work

### Single-Modality Perception

The development of various neural networks facilitates the perception of machine intelligence [27; 28; 29; 11].

**Multi-Layer Perceptron for pattern recognition.** At the beginning, support vector machine (SVM) and multi-layer perceptron (MLP) are applied to text [30], image [31], point cloud [32], and audio [33] classification. These innovative works merit the feasibility of introducing AI to pattern recognition.

**Recurrent & Convolutional Neural Network.** Hopfield Network [34] is the original form of recurrent networks, then LSTM [35] and GRU [36] further explore the advantages of RNNs in sequence modeling and application in NLP tasks [37; 38; 39], which is also widely applied in audio synthesis [40]. Meanwhile, the success of CNNs including LeNet [41], AlexNet [42], VGG [43], GoogleNet [44] and ResNet [29] in image recognition greatly promote the application of CNNs in other fields such as text classification [45; 46], point cloud understanding [47; 48; 49], and speech classification [50].

**Transformer.** Recently, transformer architecture [11] has been adopted in various tasks such as text understanding [51] and generation [52] in NLP, classification [13], detection [53] and segmentation [15] in images, point cloud understanding [22; 9], and audio recognition [6; 23].

However, similar to applications of CNNs and RNNs, these networks are modified according to distinct properties of modalities. There is no common architecture for modality-agnostic learning. More importantly, information from different modalities can be complementary [54; 55; 56], it's significant to design a framework that can encode data from different modalities and bridge these complicated representations via a shared parameter space.

### Transformed-based Multimodal Perception

The advantages of transformers for perception are the global receptive field and similarity modeling, which prominently facilitate the development of multimodal perception. MCAN [57] proposes the deep modular co-attention networks between vision and language, which performs the cross-modal alignment by concisely maximizing the cross-attention. Then it becomes a consensus [2; 1; 10; 3] to utilize a cross-attention mechanism to bridge different modalities. With the success of pretrain-finetune paradigm, more works are getting focused on how to effectively align representations extracted across modalities by pretraining. VL-BERT [58] pioneers modality-aligned representations for generic vision-language understanding with the MLM paradigm. Then Oscar [59] described the object semantics in both visual and textural contents. Frameworks such as Vinvl [60], Simvlm [1], VLMO [2], ALBEF [61], and Florence [62] further explore the advantages of joint representations across vision-language modalities in terms of semantic consistency.

Multimodal models are also utilized for few-shot learning [25], sequence-to-sequence learning [10], contrastive learning [63]. BEiT-v3 [3] proposes to take images as a foreign language with a more fine-grained cross-modal mask-and-reconstruction process, sharing partial parameters. And MoMo [64] further explores the training strategy and objective functions while using the same encoder for images and texts.

Despite these advances, there remain significant obstacles to designing unified multimodal networks due to differences between modalities. Additionally, most research in this area has focused on vision and language tasks, and may not directly contribute to challenges such as 3D point cloud understanding, audio recognition, or other modalities. The Flamingo model [25] represents a powerful few-shot learner, but its transferability to point clouds is limited, and it remains a challenge to leverage prior knowledge from one modality to benefit the others. In other means, existing multimodal methods have limited extensibility on more modalities, although they have taken expensive training costs. Addressing these discrepancies is dependent on bridging different modalities using the same set of parameters, akin to how a bridge connects multiple river banks.

## 3 Meta-Transformer

In this section, we depict the proposed framework, Meta-Transformer, in detail. Meta-Transformer unifies the multiple pipelines of processing data from different modalities and fulfills encoding texts, images, point clouds, audio, and the other 8 modalities with a shared encoder. To achieve this, Meta-Transformer is composed of a data-to-sequence tokenizer to project data to a shared embedding space, a modality-agnostic encoder to encode the embedding of different modalities, and task-specific heads to perform downstream predictions, as shown in Fig. 2.

### Preliminary

Formally, we denote the input space of \(n\) modalities as \(\{\mathcal{X}_{1},\mathcal{X}_{2},\cdots,\mathcal{X}_{n}\}\), while \(\{\mathcal{Y}_{1},\mathcal{Y}_{2},\cdots,\mathcal{Y}_{n}\}\) are the corresponding label spaces. In addition, we assume there exists an **effective** parameter space \(\Theta_{i}\) for each modality, where any parameter \(\theta_{i}\in\Theta_{i}\) can be utilized for processing data \(\mathbf{x}_{i}\in\mathcal{X}_{i}\) from that modality. We say that the essence of Meta-Transformer is to find a shared \(\theta^{*}\) that satisfies:

\[\theta^{*}\in\Theta_{1}\cap\Theta_{2}\cap\Theta_{3}\cap\cdots\Theta_{n}, \tag{1}\]

with the hypothesis:

\[\Theta_{1}\cap\Theta_{2}\cap\Theta_{3}\cap\cdots\Theta_{n}\neq\varnothing. \tag{2}\]

The multimodal neural networks can be formulated as a unified mapping function \(\mathcal{F}:\mathbf{x}\in\mathcal{X}\to\hat{y}\in\mathcal{Y}\), where \(\mathbf{x}\) is the input data coming from any modality \(\{\mathcal{X}_{1},\mathcal{X}_{2},\cdots,\mathcal{X}_{n}\}\) and \(\hat{y}\) denotes the prediction of the network. Let's denote \(y\) as the ground truth labels, the multimodal pipeline can be formulated as:

\[\hat{y}=\mathcal{F}(\mathbf{x};\theta^{*}),\ \ \theta^{*}=\operatorname*{arg\,min}_{x \in\mathcal{X}}[\mathcal{L}(\hat{y},y)]. \tag{3}\]

### Data-to-Sequence Tokenization

We propose a novel meta-tokenization scheme designed to transform data across various modalities into token embeddings, all within a shared manifold space. This approach is then applied to tokenization, taking into account the practical characteristics of modality, as illustrated in Figure 3. We take text, images, point clouds, and audio as examples. More details can be found in supplementary materials. In specific, we use \(\mathbf{x}_{T}\), \(\mathbf{x}_{I}\), \(\mathbf{x}_{P}\), and \(\mathbf{x}_{A}\) to denote a data sample of text, image, point cloud, and audio spectrogram.

**Natural Language**. Following the common practice [51; 65], we use WordPiece embeddings [66] with a 30,000 token vocabulary. WordPiece segments original words into subwords. For example, the original sentence: "The supermarket is hosting a sale", could be converted by WordPiece to: "_The _super market _is_host ing _a_sale".

In this case, the word "supermarket" is divided into two subwords "_super" and "market" and the word "hosting" is divided into "_host" and "ing", while the rest words are unchanged and still single units. The front of the first character of each original word will be stacked with a special character "_", indicating the beginning of a natural word. Each subword is corresponding to a unique token in a vocabulary, then is projected to a high-dimensional feature space with word embedding layers. As a result, each input text is transformed to a set of token embeddings \(\mathbf{x}\in\mathbb{R}^{n\times D}\), where \(n\) is the number of tokens and \(D\) is the dimension of embedding.

**Images**. To accommodate 2D images, we reshape the image \(\mathbf{x}\in\mathbb{R}^{H\times W\times C}\) into a sequence of flattened 2D patches \(\mathbf{x}_{p}\in\mathbb{R}^{N_{s}\times(S^{2}\cdot C)}\), where \((H,W)\) represents the original image resolution, \(C\) denotes the number of channels; \(S\) is the patch size, and \(N_{s}=(HW/S^{2})\) is the resulting number of patches. After that, a projection layer is utilized to project the embedding dimension to \(D\):

\[\mathbf{x}_{I}\in\mathbb{R}^{C\times H\times W}\rightarrow\mathbf{x}_{I}^{\prime}\in \mathbb{R}^{N_{s}\times(S^{2}\cdot C)}\rightarrow\mathbf{x}_{I}^{\prime\prime} \in\mathbb{R}^{N_{s}\times D}. \tag{4}\]

Note that we use the same operation for infrared images but the linear projection for hyperspectral images. In addition, we simply replace 2D convolution layers with 3D convolution for video recognition. More details can be found in B.1 and B.3.

**Point Cloud**. To learn 3D patterns with transformers, we convert point clouds from raw input space to the token embedding space. \(\mathcal{X}=\{\mathbf{x}_{i}\}_{i=1}^{P}\) denotes a point cloud of \(P\) points, where \(\mathbf{x}_{i}=(\mathbf{p}_{i},\mathbf{f}_{i})\), \(\mathbf{p}_{i}\in\mathbb{R}^{3}\) represents the 3D coordinates, and \(\mathbf{f}_{i}\in\mathbb{R}^{c}\) is feature of the \(i\)-th point. Generally, \(\mathbf{f}_{i}\) contains visual hints such as color, viewpoint, normal, etc. We employ the Farthest Point Sampling (FPS) operation to sample a representative skeleton of original point clouds with a fixed sampling ratio (1/4). Then we employ \(K\)-Nearest Neighbor (KNN) to group neighboring points. Based on grouped sets containing local geometric prior, we construct the adjacency matrix with center points of grouped subsets to further undercover the comprehensive structural information of 3D objects and 3D scenes.

Figure 2: Meta-Transformer consists of data-to-sequence tokenization, unified feature encoding, and down-stream task learning. The framework is illustrated with text, image, point cloud, and audio.

Finally, we aggregate the structural representations from \(K\) subsets. We obtain point embeddings as:

\[\mathbf{x}_{P}\in\mathbb{R}^{P\times(3+c)}\rightarrow\mathbf{x}_{P}^{\prime}\in\mathbb{R }^{\frac{P}{4}\times\frac{D}{2}}\rightarrow\mathbf{x}_{P}^{\prime\prime}\in\mathbb{ R}^{\frac{P}{16}\times D}. \tag{5}\]

**Audio Spectrogram**. Initially, we pre-process the audio waveform with the duration of \(t\) seconds with log Mel filterbank [67]. Then we employ the Hamming window with a stride of \(t_{s}\) on the frequency of \(f_{s}\) to split the original wave into \(l=(t/t_{s})\) intervals and further transform the original wave into \(l\)-dimensional filterbank.

Subsequently, we split the spectrogram into patches from time and frequency dimensions with the same patch size of \(S\). Different from image patches, audio patches overlap on spectrograms. Following AST [6], we also choose to split whole spectrograms into \(N_{s}=12[(100t-16)/10]\) patches by \(S\times S\) convolution, then we flatten patches into token sequences. Finally, we summarize the process:

\[\mathbf{x}_{A}\in\mathbb{R}^{T\times F}\rightarrow\mathbf{x}_{A}^{\prime}\in\mathbb{R}^ {N_{s}\times S\times S}\rightarrow\mathbf{x}_{A}^{\prime\prime}\in\mathbb{R}^{(N_{ s}\cdot D/S^{2})\times D}, \tag{6}\]

where \(T\) and \(F\) denote time and frequency dimensions.

### Unified Encoder

After transforming the raw inputs to token embedding space, we leverage a unified transformer encoder with frozen parameters to encode the sequences of token embeddings from different modalities.

**Pretraining**. We utilize ViT [13] as the backbone network and pre-train it on the LAION-2B dataset with contrastive learning, which reinforces the ability for generic token encoding. After pretraining, we freeze the parameters of the backbone network. In addition, for text understanding, we utilize the pretrained text tokenizer of CLIP [24] to segment sentences into subwords and transform subwords into word embeddings.

**Modality-Agnostic Learning**. Following common practice [51; 13], we prepend a learnable token \(x_{\text{CLS}}\) to the sequence of token embeddings, and the final hidden state of \(x_{\text{CLS}}\) token (\(\mathbf{z}_{L}^{0}\)) serves as the summary representation of the input sequence, which is usually utilized for performing recognition.

To reinforce positional information, we incorporate position embeddings into the token embeddings. Recall that we tokenize the input data to 1D embeddings, thus, we opt for standard learnable 1D position embeddings. In addition, we do not observe substantial performance improvements using more sophisticated 2D-aware position embeddings on image recognition. We simply fuse the position embeddings and the content embeddings with an element-wise addition operation, and the resulting embedding sequences are then fed into the encoder.

The transformer encoder with a depth of \(L\) compromises multiple stacked multi-head self-attention (MSA) layers and MLP blocks. The input token embeddings are fed into an MSA layer first, then

Figure 3: Illustration of Data-to-Sequence Tokenization 3.2. We propose the meta scheme in (a) containing grouping, convolution, and transformation progress. Then (b)-(e) represents the building blocks applied with our meta scheme on texts, images, point clouds, and audio spectrograms.

an MLP block. Then the output of \((\ell-1)\)-th MLP block serves as the input of \(\ell\)-th MSA layer. Layer Normalization (LN) is appended before each layer and the residual connection is applied after each layer. The MLP contains two linear FC layers along with a GELU non-linear activation. The formulation of the transformer is:

\[\mathbf{z}_{0} =[\mathbf{x}_{\texttt{CLS}};\,\mathbf{E}_{\mathbf{x}_{1}};\,\mathbf{E}_{\mathbf{x}_{2}}; \cdots;\,\mathbf{E}_{\mathbf{x}_{n}}]+\mathbf{E}_{pos}, \mathbf{E}\in\mathbb{R}^{n\times D},\,\mathbf{E}_{pos}\in\mathbb{R}^{(n+1) \times D} \tag{7}\] \[\mathbf{z}_{\ell}^{\prime} =\operatorname{MSA}(\texttt{LN}(\mathbf{z}_{\ell-1}))+\mathbf{z}_{\ell-1}, \ell =1\ldots L\] (8) \[\mathbf{z}_{\ell} =\operatorname{MLP}(\texttt{LN}(\mathbf{z}_{\ell}^{\prime}))+\mathbf{z}_{ \ell}^{\prime}, \ell =1\ldots L\] (9) \[\mathbf{y} =\texttt{LN}(\mathbf{z}_{L}^{0}) \tag{10}\]

where \(\mathbf{E}_{x}\) denotes the token embeddings from proposed tokenizer and \(n\) denotes the number of tokens. We augment patch embeddings and learnable embedding with position embeddings \(\mathbf{E}_{pos}\).

### Task-Specific Heads

After obtaining learning representations, we feed representations to the task-specific heads \(h(\cdot;\theta_{h})\), which consists mainly of MLPs and varies from modalities and tasks. The learning objective of Meta-Transformer can be summarized as:

\[\hat{\mathbf{y}}=\mathcal{F}(\mathbf{x};\theta^{*})=h\circ g\circ f(\mathbf{x}),\;\;\; \theta^{*}=\operatorname*{arg\,min}_{\theta}\mathcal{L}(\hat{y},y), \tag{11}\]

where \(f(\cdot)\), \(g(\cdot)\), and \(h(\cdot)\) denote the function of tokenizer, backbone, and heads, respectively.

## 4 Experiments

In this section, we perform experiments on each of the 12 modalities. We demonstrate the potential of Meta-Transformer for multimodal perception. A summary of our experimental design is shown in Table 2 and more experimental details can be found in SS C.1.

### Experimental Setups

**Text understanding**. For text understanding evaluation, we employ the General Language Understanding Evaluation (GLUE) benchmark [68] which incorporates several different datasets, covering a wide range of natural language understanding tasks.

**Image understanding**. 1) Classification: we conduct experiments on ImageNet-1K [69] which contains approximately 1.3 million images with 1000 categories. Following common practices [70; 19; 71], base-scale models are trained for 300 epochs, while large models are pre-trained on ImageNet-22K (14.2 million images) for 90 epochs and fine-tuned on ImageNet-1K for another 20 epochs. 2) Object Detection: we conduct experiments on the MS COCO dataset [72] using Mask R-CNN [73] as the detector and training each model for 12 epochs. 3) Semantic Segmentation: we train the segmentation head UperNet [74] on ADE20K [75] for 160k iterations, providing a fair comparison with previous CNN-based and transformer-based backbones.

**Infrared, X-Ray, and Hyperspectral data understanding**. We conduct experiments on infrared image, X-Ray scan, and hyperspectral data recognition with RegDB [76], Chest X-Ray [77], and Indian Pine 4 datasets, respectively.

Footnote 4: [https://github.com/danfenghong/IEEE_TGRS_SpectralFormer/blob/main/data/IndianPine.mat](https://github.com/danfenghong/IEEE_TGRS_SpectralFormer/blob/main/data/IndianPine.mat)

**Point cloud understanding**. 1) Classification: to assess the performance of Meta-Transformer in 3D object classification, **we** use the ModelNet-40 [78] benchmark, consisting of CAD models across 40 classes, with 9,843 training samples and 2,468 validation samples. 2) Semantic segmentation: to evaluate performance in 3D point cloud segmentation, we assess the model on both S3DIS [79] and ShapeNetPart [80] datasets. The S3DIS dataset encompasses 6 large indoor areas and 13 semantic classes, comprising 271 rooms. The ShapeNetPart dataset includes 16,880 object models across 16 shape categories.

**Audio recognition**. For audio recognition, we utilize the Speech Commands V2 [81] dataset, which consists of 105,829 one-second recordings of 35 common speech commands.

**Video recognition**. For video understanding, we conduct experiments on the UCF101 [82] dataset for action recognition, with more details presented in SS B.1.

**Time-series forecasting**. For time-series forecasting, we conduct experiments on ETTh1 [83], Traffic5, Weather6, and Exchange [84] datasets. We use the tokenizer of Autoformer [85].

Footnote 5: [https://pems.dot.ca.gov/](https://pems.dot.ca.gov/)

Footnote 6: [https://www.bgc-jena.mpg.de/wetter/](https://www.bgc-jena.mpg.de/wetter/)

**Graph understanding**. We conduct experiments on the PCQM4M-LSC dataset [86], which is a large-scale dataset consisting of 4.4 million organic molecules with up to 23 heavy atoms with their corresponding quantum-mechanical properties. With the target of predicting molecular properties using machine learning, it has plenty of applications in drug discovery, and material science.

**Tabular analysis**. We conduct experiments on adult and bank marketing from UCI repository 7. We use the tokenizer of TabTransformer [87] to encode raw tabular data.

Footnote 7: [http://archive.ics.uci.edu/ml/](http://archive.ics.uci.edu/ml/)

**IMU recognition**. To evaluate the ability of Meta-Transformer to understand the inertial motion systems, we conduct experiments of IMU sensor classification on the Ego4D [88] dataset.

**Settings of Networks**: We follow the default settings of ViT [13]. Meta-Transformer-B16\({}_{F}\) denotes Meta-Transformer with a base-scale encoder which contains 12 transformer blocks and 12 attention heads, and the image patch size is 16. For the base-scale encoder, the embedding dimension is 768 and the output dimension of MLP is 3,072. 'F' and 'T' denotes that parameters of the encoder are _Frozen_ and further _Tuned_, respectively.

$$
\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline Modalities & Tasks & Datasets & Data Scale \\ \hline \(\oplus\) Text & Classification & GLUE Benchmark & 330K \\ \hline \multirow{3}{*}{Image} & Classification & ImageNet-1K & 1.3M \\  & Detection & MS COCO & 118K \\  & Segmentation & ADE-20K & 20K \\ \hline \multirow{3}{*}{Point Cloud} & Shape Classification & ModelNet-40 & 9K \\  & Scene Segmentation & S3DIS & 400M Points \\  & Object Segmentation & ShapeNetPart & 16K \\ \hline \multirow{3}{*}{Video} & Classification & Speech commands v2 & 105K \\  & Action Recognition & UCF101 & 14K \\ \hline \(\oplus\) Infrared & Classification & RegDB & 40K \\ \hline \(\rtimes\) Hyper-spectrum & Classification & Indian Pine & 10K \\ \hline \(\rtimes_{\text{E}}\) X-Ray & Classification & Chest X-Ray & 112K \\ \hline \(\rtimes\) IMU & Classification & Ego4D & 193K \\ \hline \(\rtimes\) Tabular data & Prediction & Adult \& Bank & 32K-45K \\ \hline \(\rtimes\) Graph data & Prediction & PCQM4M-LSC & 47M \\ \hline \(\rtimes\) Time-series & Forecasting & Exchange, Traffic, _etc_ & 5-36K \\ \hline \hline \end{tabular}
\end{table}
$$
Table 2: Summary of experimental settings across different modalities. We report the task, dataset, and data scale for each modality.

\begin{table}
\begin{tabular}{l|c|c|c|c|c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c|}{Pertraining Settings} & \multicolumn{4}{c}{GLUE Benchmark} \\ \cline{2-9}  & Modality & Data & Size & SST-2 & MRPC & QQP & MNLI & QNLI \\  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{}{} & \multicolumn{1}{c|}{}{} \\  & \multicolumn{1}{c|}{}{} \multicolumn{1}{c|}{}{} \multicolumn{1}{|}{}} & \multicolumn{1}{c|}{}{} & \multicolumn{1}{|}{}{} & \multicolumn{1}{|}{|}{} & \multicolumn{1}{|}{|}{} & \multicolumn{1}{|}{|}{} & \multicolumn{1}{|}{|}{} \\ \hline BiLSTM+ELMoF+Attn & - - & - & - & - & 90.4 & 84.9 & 64.8 & 76.4 & 79.8 \\ \hline OpenAI GPT [89] & & \multirow{3}{*}{Language}{}{} & Book & 0.8B & 91.3 & 82.3 & 70.3 & 82.1 & 87.4 \\ \cline{2-2} BERT\({}_{\text{base}}\)[51] & & & & 88.0 & 88.9 & 71.2 & 84.6 & 90.5 \\ \cline{2-2} RoBERT\({}_{\text{base}}\)[65] & & & & **96.0** & **90.0** & **84.0** & 84.0 & **92.0** \\ ChaGPT & \multicolumn{1}{c|}{\multirow{2}{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{}}} & \multirow{2}{*{}}} & \multicolumn{1}{c}{}} & \multicolumn{1}{c}{}} & \multicolumn{1}{c}{}{} \\ \multicolumn{1}{c}{}{}{} & & & & & & & & \\  & & & & & & & \\  & & & & & & & \\  & & & & & & & \\  & & & & & & & \\  & & & & & & & \\  & & & & & & & \\  & & & & & & & &

### Results on Natural Language Understanding

Table 3 illustrates the experimental results on the GLUE benchmark for text understanding tasks, comparing various state-of-the-art methods such as BERT [51], RoBERTa [65], and ChatGPT. The comparison centers on paraphrasing, sentiment, duplication, inference, and answering tasks. When using frozen parameters pretrained on images, Meta-Transformer-B16 achieves scores of 54.6% in sentiment (SST-2), 81.1% in paraphrase (MRPC), 66.0% in duplication (QQP), 63.4% in inference (MNLI), and 56.3% in answering (QNLI) tasks. After finetuning, Meta-Transformer-B16T exhibits improved performance, with 81.3% in sentiment, 81.8% in paraphrase, 78.0% in duplication, 70.0% in inference, and 60.3% in answering tasks. Although the Meta-Transformer's performance on the GLUE benchmark might not be as impressive as that of BERT, RoBERTa, or ChatGPT, it still demonstrates competitive performance, adaptability, and potential for understanding natural language.

### Results on Image Understanding

As shown in Table 4, Meta-Transformer exhibits outstanding performance when compared with Swin Transformer series [19; 107] and InternImage [96] on image understanding tasks. On image classification, with the help of CLIP [24] text encoder, Meta-Transformer delivers great performances under zero-shot classification with the Meta-Transformer-B16 and Meta-Transformer-L14\({}_{\text{F}}\), achieving 69.3% and 75.3%, respectively. At the same time, when the pretrained parameters are further tuned, Meta-Transformer can outperform existing advanced methods, with Meta-Transformer-B16\({}_{\text{T}}\) and Meta-Transformer-L14\({}_{\text{T}}\) achieving 85.4% and 88.1% accuracy, respectively. The latter outperforms both SwinV2-L/24\({}^{\dagger}\)[107] (87.6%) and InternImage-XL [96]\({}^{\dagger}\) (88.0%) on ImageNet [69] classification.

\begin{table}
\begin{tabular}{l|c c c|c c c|c c|c c} \hline \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Classification} & \multicolumn{3}{c}{Object Detection} & \multicolumn{3}{c}{Semantic Segmentation} \\ \cline{2-13}  & Res & \#Params & \#FLOPs & Acc (\%) & \#Params & GFLOPs & \#Params & GFLOPs & mIoU (\%) \\ \hline PVT-L [70] & \(224^{\ddagger}\) & 61.4M & 9.86 & 81.7 & 81.0M & - & 42.9 & 65.1M & 79.6G & 44.8 \\ Swin-L\({}^{\dagger}\)[19] & \(384^{\ddagger}\) & 197M & 104G & 87.3 & 253M & 1382G & 515.8 & 234M & 2468G & 52.1 \\ CoAtt-3\({}^{\dagger}\)[90] & \(384^{\ddagger}\) & 168M & 107G & 87.6 & - & - & - & - & - \\ CoAttNet-4\({}^{\dagger}\)[90] & \(384^{\ddagger}\) & 275M & 190G & 87.9 & - & - & - & - & - \\ DeiT III-L\({}^{\dagger}\)[91] & \(384^{\ddagger}\) & 2304M & 191G & 87.7 & - & - & - & 353.6M & 2231G & 51.5 \\ SwinV2-L24\({}^{\dagger}\)[92] & \(384^{\ddagger}\) & 197M & 115G & 87.6 & - & - & **58.8** & - & - & **55.9** \\ ReplKR-NIC-31\({}^{\dagger}\)[193] & \(384^{\ddagger}\) & 172M & 966M & 86.6 & 2292M & 1321G & 53.9 & 207M & 2404G & 52.4 \\ HorNet+L\({}^{\dagger}\)[94] & \(384^{\ddagger}\) & 202M & 102G & 87.7 & 259M & 1358G & 56.0 & 232M & 2473G & 54.1 \\ ConvNet-L\({}^{\dagger}\)[95] & \(384^{\ddagger}\) & 198M & 101G & 87.5 & 255M & 1354G & 53.5 & 235M & 2458G & 535.2 \\ ConvNet-XL-[95] & \(384^{\ddagger}\) & 3530M & 1795M & 87.8 & 407M & 1898G & 53.6 & 391M & 3335G & 53.6 \\ InterImage-L\({}^{\dagger}\)[96] & \(384^{\ddagger}\) & 223M & 108G & 87.7 & 277M & 1399G & 54.9 & 256M & 2526G & 53.9 \\ InterImage-XL\({}^{\dagger}\)[96] & \(384^{\ddagger}\) & 3353M & 16363 & 85.0 & 38M & 37178G2G & 55.53 & 368M & 3142G & 555.0 \\ Meta-Transformer-B16\({}^{\dagger}\)[96] & \(224^{\ddagger}\) & 86.6M & 17.5G & 69.3\({}^{\ddagger}\) & 143M & 1126G & 31.7 & 164M & 135G & 33.4 \\ \hline Meta-Transformer-L14\({}^{\dagger}\)[90] & \(224^{\ddagger}\) & 86.6M & 175.G & 69.3\({}^{\ddagger}\) & 143M & 1126G & 31.7 & 164M & 135G & 33.4 \\ \hline Meta-Transformer-L14\({}^{\dagger}\)[90] & \(336^{\ddagger}\) & 191.M & 1906.6M & 75.3\({}^{\ddagger}\) & 364M & 2143G & 43.5 & 314M & 683G & 41.2 \\ \hline Meta-Transformer-Bl6\({}^{\dagger}\)[10000] & \(224^{\ddagger}\) & 86.M & 17.5G & 85.4 & 143M & 1126G & 46.4 & 164M & 135G & 48.3 \\ Meta-Transformer-L14\({}^{\dagger}\) &336\(1\) & \(191.1M\) & 190.6G & 88.1\({}^{\dagger}\) & & 364M & 2143G & 56.3 & 314M & 683G & 41.2 \\ \hline Meta-Transformer-Bl6\({}^{\dagger}\)[10000] & \(224^{\ddagger}\) & 86.M & 17.5G & 85.4 & 143M & 1126G & 46.4 & 164M & 135G & 48.3 \\ Meta-Transformer-L14\({}^{\dagger}\) &336 &191.1M & 190.6G & **88.1** & 364M & 2143G & 56.3 & 314M & 683G & 55.0 \\ \hline \multicolumn{{1}{1}{1}{1}{1}

* \(\({}^{\star}\): zero-shot classification -shot classification & \({}{}^{\dagger}\):\): linear probing for classification & \(\): \(\)\(\)\(\ddagger\): \(\dd\) models pre-trained on ImageNet-222K \\ \enderingMode} \({smallmatrix}\)\({smallmatrix}\): \({}\): zero-shot classification -shot classification \({}^{\dagger}\): : :\) linear probing for classification \(\(\)\)\({}^{\ddagger}\): \(\):\(\) models pre-trained on ImageNet222K \\ \({}^{\dagger\)}\) &336^\ddagger}\) & 336^\ddagger}\) & 191.M & 190.6G & 88.1 & 364M & 2143G & 56.3 & 314M & 683G & 55.0 \\ \hline Meta-Transformer-Bl6\({}^{\dagger}\)[10000] & \(224^{\ddagger}\) & 86.0M & 17.5G & 85.4 & 143M & 1126G & 46.4 & 1643G & 135G & 135G & 135G & 15G & 15G & 15G & 15G &When it comes to object detection and semantic segmentation, Meta-Transformer also delivers excellent performances, which further proves its generic ability on image understanding. On object detection, Meta-Transformer-B16\({}_{\text{F}}\) and Meta-Transformer-L14\({}_{\text{F}}\) achieve APs of 31.7% and 43.5%, while Meta-Transformer-B16\({}_{\text{T}}\) and Meta-Transformer-L14\({}_{\text{T}}\) reach 46.4% and 56.3% AP, respectively. In semantic segmentation, the mIoUs for Meta-Transformer-B16\({}_{\text{F}}\) and Meta-Transformer-L14\({}_{\text{F}}\) are 33.4% and 41.2%, while Meta-Transformer-B16\({}_{\text{T}}\) and Meta-Transformer-L14\({}_{\text{T}}\) achieve 51.0% and 55.0%, respectively. In comparison, SwinV2-L/24\({}^{\ddagger}\) outperforms the Meta-Transformer in both object detection (58.8% AP) and semantic segmentation (55.9% mIoU). The Meta-Transformer-L14\({}_{\text{T}}\) model has a similar performance to InterImage-XL\({}^{\ddagger}\)[96] in semantic segmentation (both achieving 55.0% mIoU), but outperforms it in object detection (56.3% AP compared to 55.3% AP). These results highlight that Meta-Transformer demonstrates a competitive performance in various image understanding tasks even compared to Swin Transformer [19] and InterImage.

### Results on Infrared, Hyperspectral, and X-Ray data

Table 4(a) presents the performance comparison of Meta-Transformer and other advanced methods on the RegDB dataset [76] for infrared image recognition. Meta-Transformer-B16\({}_{\text{F}}\) demonstrates competitive results with a Rank-1 accuracy of 73.50% and an mAP of 65.19%. While it may not outperform the top-performing methods, Meta-Transformer proves to be a simple transferable approach for infrared image recognition tasks. These results indicate the potential of Meta-Transformer in handling the challenges associated with infrared images and contribute to advancements in this field.

In addition, Table 4(b) presents the performance of Meta-Transformer on the Indian Pine dataset for hyperspectral image recognition. SpectralFormer [100] achieves impressive accuracy scores, with a patch-wise approach. Plain vision transformer also performs well in comparison when fully tuning all parameters. Meta-Transformer-B16\({}_{\text{F}}\) demonstrates competitive results on hyperspectral image recognition with lower overall accuracy. However, Meta-Transformer metrics (only 0.17M) compared to other methods. This reveals a promising development direction of applying the Meta-Transformer to remote sensing, environmental monitoring, and mineral exploration. For X-Ray images, similar to dealing with infrared images, we take the same image tokenizer as common visible images. From Table 7, we can observe that Meta-Transformer can achieve a competitive performance of 94.1% accuracy.

### Results on 3D Point Cloud Understanding

Table 6 showcases the experimental results for point cloud understanding, comparing the performance of Meta-Transformer with other state-of-the-art methods on the ModelNet-40 [78], S3DIS [79],

\begin{table}
\begin{tabular}{l|c c c|c c c|c c c} \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Pre-train} & \multicolumn{3}{c|}{ModelNet-40} & \multicolumn{3}{c}{S3DIS Area-5} & \multicolumn{3}{c}{S3DIS/Part} \\ \cline{3-10}  & & & max (\%) & OA (\%) & Params & & mIoU (\%) & Max (\%) & Params & mIoU (\%) & mIoU (\%) & Params \\ \hline PointNet conv1 [32] & N/A & 86.0 & 89.2 & 3.5M & 41.1 & 49.0 & 3.6M & 83.7 & 80.4 & 3.6M \\ PointNet conv2 [5] & N/A & - & 91.9 & 1.5M & 53.5 & - & 1.0M & 85.1 & 81.9 & 1.0 \\ PointCNN conv1 [47] & N/A & 88.1 & 92.5 & 0.6M & 57.3 & - & 0.6M & & & \\ FCPConv1 [49] & N/A & - & 92.9 & 14.3M & 67.1 & 72.8 & 15.0M & 86.4 & 85.1 & - \\ DGCNN conv2 [101] & N/A & 90.2 & 92.9 & 1.8M & 52.5 & - & 1.3M & 85.2 & 82.3 & 1.3 \\ Point Transformer conv3 [91] & N/A & 90.6 & 93.7 & 7.8M & 70.4 & - & 7.8M & 86.6 & 83.7 & 7.8 \\ PointNet conv1 [102] & N/A & 90.8 & 93.2 & 1.4M & 67.3 & 73.9 & 3.8M & 86.7 & 84.4 & 1.0 \\ Point-MLP conv2 [103] & N/A & 90.9 & 93.6 & 0.6M & - & - & - & 86.1 & 84.6 & - \\ PointNet conv1 [104] & N/A & 91.4 & 93.6 & 3.6M & 71.4 & 77.4 & 6.5M & - & - & - \\ PointNet conv2 [102] & 3D & - & - & 93.2 & 21.1M & 60.8 & 69.9 & 21.1M & 85.6 & 84.1 & 21.1M \\ PointNet conv1 [105] & 3D & - & **93.8** & 21.1M & - & - & - & 86.1 & 84.2 & 21.1M \\ P2P [06] & 2D & - & 93.1 & 1.2M & - & - & - & 86.5 & 84.1 & - \\ ACT conv1 [106] & 2D & - & 93.5 & 21.1M & 61.2 & 71.1 & 21.1M & 86.1 & 84.7 & 21.2M \\ \hline Meta-Transformer-B16\({}_{\text{F}}\) just & 2D & 90.3 & 59.6 & 0.6M & **72.3** & **80.5** & 2.3M & **87.0** & **85.2** & 2.3M \\ \hline \end{tabular}
\end{table}
Table 6: **Experimental results for point cloud understanding. We conduct experiments on the ModelNet-40 [78], S3DIS [79],794.6 and ShapeNetPart[80] datasets. We compare existing advanced methods from classification, semantic, and object part segmentation tasks, and we report the pre-training modality (Pre-train) and **training modality (Pre-train) and ** modality parameters number of trainable parameters.

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline \multicolumn{1}{c}{}{}{} & & \multicolumn{1}{c}{}{}{} & \\ \multicolumn{1}{c}{}{}{} & \multicolumn{1}{c}{}{}{} & \\ \multicolumn{1}{c}{}{}{} }{} & & \multicolumn{1}{c}{}{}{}} \\ \multicolumn{1}{c}{}{}{} } & & \multicolumn{1}{c}{}{}{} } \\ \multicolumn{1}{c}{}{}{} & } & & \multicolumn{1}{c}{}{}{}{} \\ \multicolumn{1}{}{}{}{} }{& } & \multicolumn{1}{c}{}{}{}{}} & \\ \multicolumn{1}{c}{}{}{}{}{} } & } & & \multicolumn{1}{c}{}{}{}{}{}} & \\ \multicolumn{}{}{}{}{}{}} {\multicolumn{1}{c}{}{}{}{}} } & {\multicolumn{}{}{}{}{}{}} & {\multicolumn{1}{c}{}{}{}{}}{}} \\ & {\}{\}{\}{\multicolumn{}{}{}{}{}{}{}}{\multicolumn{1}{c}{}{}{}}{}{}}{ } \\ & & \multicolumn{{1}{}{c}}{}}{{}}{& } {\multicolumn{{}{}{}{}{}}{}{}}{} & & {\multicolumn{1}{c}}}{}{}}{ {}} & & \\ \multicolumn{{}{}{}}{}{} & {\multicolumn{1}{c}}}{}{}{} & & \\ \multicolumn{}{}{}{}{\multicolumn{}{}{}{}{}} & & & \multicolumn{1}{c}}}{} & & \\ \multicolumn{}{\multicolumn{}{}{}{}{}}{\multicolumn{}{}{}{}{}} & & \multicolumn{1}{c}}{}{\mul}{\multicolumn{}{}{}{}} & \\ \multicolumn{}{\multicolumn{}{}{}{}{}} & & \multicolumn{1}{c}}{\multicolumn{}{}{\mul}{}{\mul}{\mul}{\multicolumn{}{}{}{}} & & \multicolumn{}{}}{\multicolumn{}{\multicolumn{}{}{\multicolumn{1}{c}}{\mul}{\multicolumn{}{\multicolumn{}}{\multicolumn{}}{\multicolumn{\multicolumn{}}{\multicolumn{\}}{\multicolumn{\}}}{\multicolumn{\multicolumn{1}{\multicolumn{1}{c}{}{}{}{}{}{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}, + + + ,  + and ShapeNetPart [80] datasets. The tasks include classification, semantic segmentation, and object part segmentation. When pretrained on 2D data, Meta-Transformer-B16\({}_{\text{F}}\) demonstrates competitive performance, achieving an overall accuracy (OA) of 93.6% on ModelNet-40 with only 0.6M trainable parameters, which is comparable to the best-performing models. On the S3DIS Area-5 dataset, Meta-Transformer outperforms other methods with a mean IoU (mIoU) of 72.3% and a mean accuracy (mAcc) of 83.5%, using 2.3M parameters. Moreover, Meta-Transformer excels in the ShapeNetPart dataset, achieving the highest scores on both instances mIoU (mIoU\({}_{I}\)) and category mIoU (mIoU\({}_{C}\)) with 87.0% and 85.2%, respectively, using 2.3M parameters. In summary, Meta-Transformer demonstrates remarkable advantages in point cloud understanding tasks, offering competitive performance with fewer trainable parameters compared to other state-of-the-art methods.

### Results on Audio Recognition

In order to fairly compare Meta-Transformer with existing audio transformer series [6; 23] of similar scale, we conduct experiments on audio recognition using Meta-Transformer-B32. Table 8 showcases the performance of Meta-Transformer in the audio domain. These models are compared to existing methods such as AST [6] and SSAST [23] in terms of accuracy, all parameters (A-Params), and trainable parameters (T-Params). With frozen parameters, Meta-Transformer-B32F achieves an accuracy of 78.3% while requiring only 1.1M parameters for tuning. On the other hand, the Meta-Transformer-B32T achieves across different fields.

### Results on Video Recognition

Table 9 presents the performance comparison of the Meta-Transformer and existing advanced methods on the UCF101 dataset for video understanding. Several state-of-the-art video-tailored methods achieve accuracies of over 90%. Meta-Transformer only contains a negligible amount of trainable parameters of 1.1 million to obtain an accuracy of 46.6% while other methods have to train around 86.9 million parameters. Though Meta-Transformer is not able to beat other state-of-the-art video understanding models, Meta-Transformer stands out for its significantly reduced trainable parameter count, suggesting the potential benefit of unified multi-modal learning and less architectural complexity.

### Results on Time-series Forecasting

To explore the ability of Meta-Transformer for time-series forecasting, we conduct experiments on several widely-adopted benchmarks for Long-term forecasting tasks including ETTh1 [83], Traffic, Weather, and Exchange [84], with results shown in Table 10.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline
**Method** & Pre-train & **Acc (\%)** & **A-Params** & **Params** \\ \hline AST [6] (Supervised) & N/A & 92.6 & 86.9M & 86.9M \\ AST [6] (Supervised) & AudioSet-20K & 96.2 & 86.9M & 86.9M \\ AST [6] (Supervised) & ImageNet+KD & **98.1** & 86.9M & 86.9M \\ SSAST [23] (Self-Supervised) & AudioSet-2M & 97.8 & 89.3M & 89.3M \\ SSAST [23] (Self-Supervised) & Librispeech & 97.8 & 89.3M & 89.3M \\ SSAST [23] (Self-Supervised) & Joint Pretraining & 98.0 & 89.3M & 89.3M \\ \hline Meta-Transformer-B32\({}_{\text{F}}\) & 2D & 78.3 & 86.6M & **1.1M** \\ Meta-Transformer-B32\({}_{\text{F}}\) & 2D & 97.0 & 86.6M & 86.3M \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Audio understanding with Meta-Transformer**. We conduct experiments on the Speech Commands V2 dataset and report the accuracy score and the number of trainable and all parameters.

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline
**Method** & **Modality** & **UCF101** & **Params** \\ \hline OPN [109] & V & 59.6 & - \\ SimCLR [110] & V & 88.9 & 86.9M \\ VideoMAE V1 [111] & V & 96.1 & 86.9M \\ VideoMAE V2 [112] & V & **99.6** & 86.9M \\ \hline ViT [13] (from scratch) & V & 51.4 & 86.9M \\ Meta-Transformer-B16\({}_{\text{F}}\) & V & 46.6 & **1.1M** \\ \hline \hline \end{tabular}
\end{table}
Table 9: **Video understanding with Meta-Transformer**. We conduct experiments on the UCF101 dataset and the number of trainable parameters, where “V” denotes video clips only.

data understanding, especially on complex datasets such as Bank Marketing.

### Results on Graph and IMU Data Understanding

We report the performance of utilizing Meta-Transformer for graph understanding in Table 12. We compare Meta-Transformer-B16\({}_{\text{F}}\) with various graph neural network models for graph data understanding on the PCQM4M-LSC dataset [86]. Among all the methods, Graphormer shows the best performance with the lowest train and validation MAE scores of 0.0582 and 0.1234, respectively.

In contrast, Meta-Transformer-B16F delivers the train and validation MAE scores of 0.8034 and 0.8863, which reveals the limited ability of current Meta-Transformer architecture for structural data learning. We will further improve this in the future. Besides, following ImageBind [26], we conduct classification on the Ego4D dataset [88], with input data, Meta-Transformer delivers an accuracy of 73.9%.

## 5 Limitation

From the perspectives of complexity, methodology, and further application, the limitations of the Meta-Transformer are summarized as follows:

**Complexity**: Meta-Transformer requires \(\mathcal{O}(n^{2}\times D)\) computation dealing with token embeddings \([E_{1},\cdots,E_{n}]\). High memory cost and heavy computation burden make it difficult to scale up.

**Methodology**: Compared with Axial Attention mechanism in TimeSformer [7] and Graphormer [125], Meta-Transformer lacks temporal and structural awareness. This limitation may affect the overall performance of Meta-Transformer in tasks where temporal and structural modeling plays a critical role, such as video understanding, visual tracking, or social network prediction.

**Application**: Meta-Transformer primarily delivers its advantages in multimodal perception. It's still unknown about its ability for cross-modal generation. We will work on this in the future.

## 6 Conclusion

In the early stages of artificial intelligence development, pioneers introduced the Multi-Layer Perceptron (MLP) to address prediction tasks in machine learning. Later, recurrent and convolutional networks expanded AI capabilities in multimedia data processing, achieving significant success in extracting representations from texts, images, point clouds, and audio. MLPs have since been integrated into deep convolutional networks. In this paper, we explore the potential of plain transformers for unified multimodal learning, highlighting a promising trend toward developing unified multimodal intelligence with a transformer backbone. To some extent, this paper supports the dominant position of transformers in next-generation networks. Importantly, CNNs and MLPs are not left behind. They play essential roles in data tokenization and representation projection. This process exemplifies the law of succession in neural networks and the ongoing evolution of artificial intelligence.

## References

* [1]Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. _arXiv preprint arXiv:2108.10904_, 2021.
* [2] Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. VLmo: Unified vision-language pre-training with mixture-of-modality-experts. _arXiv preprint arXiv:2111.02358_, 2021.
* [3] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. _arXiv preprint arXiv:2208.10442_, 2022.
* [4] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16000-16009, 2022.
* [5] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In _NeurIPS_, 2017.
* [6] Yuan Gong, Yu-An Chung, and James Glass. Ast: Audio spectrogram transformer. _arXiv preprint arXiv:2104.01778_, 2021.
* [7] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In _Proceedings of the International Conference on Machine Learning (ICML)_, July 2021.
* [8] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International Conference on Machine Learning_, pages 1263-1272. PMLR, 2017.
* [9] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In _ICCV_, pages 16259-16268, 2021.
* [10] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. _arXiv preprint arXiv:2202.03052_, 2022.
* [11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [12] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In _ECCV_, 2020.
* [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. _ICLR_, 2021.
* [14] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In _CVPR_, pages 12104-12113, 2022.
* [15] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. _Advances in Neural Information Processing Systems_, 34:12077-12090, 2021.
* [16] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. _arXiv:2106.13797_, 2021.

* [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.
* [18] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. _arXiv preprint arXiv:2205.08534_, 2022.
* [19] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _ICCV_, pages 10012-10022, 2021.
* [20] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In _CVPR_, 2022.
* [21] Guocheng Qian, Xingdi Zhang, Abdullah Hamdi, and Bernard Ghanem. Pix4point: Image pretrained transformers for 3d point cloud understanding. _arXiv preprint arXiv:2208.12259_, 2022.
* [22] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min Hu. Pct: Point cloud transformer. _Computational Visual Media_, 7(2):187-199, 2021.
* [23] Yuan Gong, Cheng-I Lai, Yu-An Chung, and James Glass. Ssast: Self-supervised audio spectrogram transformer. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 10699-10709, 2022.
* [24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, pages 8748-8763. PMLR, 2021.
* [25] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. _arXiv preprint arXiv:2204.14198_, 2022.
* [26] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15180-15190, 2023.
* [27] Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity. _The bulletin of mathematical biophysics_, 5:115-133, 1943.
* [28] Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support vector machines. _IEEE Intelligent Systems and their applications_, 13(4):18-28, 1998.
* [29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, pages 770-778, 2016.
* [30] Zhao Xu, Kai Yu, Volker Tresp, Xiaowei Xu, and Jizhi Wang. Representative sampling for text classification using support vector machines. In _Advances in Information Retrieval: 25th European Conference on IR Research, ECIR 2003, Pisa, Italy, April 14-16, 2003. Proceedings 25_, pages 393-407. Springer, 2003.
* [31] Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, and Lawrence Jackel. Handwritten digit recognition with a back-propagation network. _Advances in neural information processing systems_, 2, 1989.
* [32] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In _CVPR_, 2017.
* [33] P Dhanalakshmi, S Palanivel, and Vennila Ramalingam. Classification of audio signals using svm and rbfnnn. _Expert systems with applications_, 36(3):6069-6075, 2009.

* [34] John J Hopfield. Neural networks and physical systems with emergent collective computational abilities. _Proceedings of the national academy of sciences_, 79(8):2554-2558, 1982.
* [35] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.
* [36] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. _arXiv preprint arXiv:1412.3555_, 2014.
* [37] Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. Abstractive text summarization using sequence-to-sequence rnns and beyond. _arXiv preprint arXiv:1602.06023_, 2016.
* [38] Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. _arXiv preprint arXiv:1406.1078_, 2014.
* [39] Duyu Tang, Bing Qin, and Ting Liu. Document modeling with gated recurrent neural network for sentiment classification. In _Proceedings of the 2015 conference on empirical methods in natural language processing_, pages 1422-1432, 2015.
* [40] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural audio synthesis. In _International Conference on Machine Learning_, pages 2410-2419. PMLR, 2018.
* [41] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [42] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Communications of the ACM_, 60(6):84-90, 2017.
* [43] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In _ICLR_, 2015.
* [44] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1-9, 2015.
* [45] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. _Advances in neural information processing systems_, 28, 2015.
* [46] Ye Zhang and Byron Wallace. A sensitivity analysis of (and practitioners' guide to) convolutional neural networks for sentence classification. _arXiv preprint arXiv:1510.03820_, 2015.
* [47] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. _Advances in neural information processing systems_, 31, 2018.
* [48] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In _IROS_, 2015.
* [49] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Francois Goulette, and Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point clouds. In _ICCV_, 2019.
* [50] Ossama Abdel-Hamid, Abdel-rahman Mohamed, Hui Jiang, Li Deng, Gerald Penn, and Dong Yu. Convolutional neural networks for speech recognition. _IEEE/ACM Transactions on audio, speech, and language processing_, 22(10):1533-1545, 2014.
* [51] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _NAACL-HLT_, 2019.

* [52] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [53] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I 16_, pages 213-229. Springer, 2020.
* [54] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela Rus, and Song Han. Bevfusion: Multi-task multi-sensor fusion with unified bird's-eye view representation. _arXiv preprint arXiv:2205.13542_, 2022.
* [55] Feihu Zhang, Jin Fang, Benjamin Wah, and Philip Torr. Deep fusionnet for point cloud semantic segmentation. In _ECCV_, 2020.
* [56] Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, and Jiwen Lu. P2p: Tuning pre-trained image models for point cloud analysis with point-to-pixel prompting. _arXiv preprint arXiv:2208.02812_, 2022.
* [57] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for visual question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6281-6290, 2019.
* [58] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations. _arXiv preprint arXiv:1908.08530_, 2019.
* [59] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In _European Conference on Computer Vision_, pages 121-137. Springer, 2020.
* [60] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5579-5588, 2021.
* [61] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. _Advances in neural information processing systems_, 34:9694-9705, 2021.
* [62] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. _arXiv preprint arXiv:2111.11432_, 2021.
* [63] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. _arXiv preprint arXiv:2205.01917_, 2022.
* [64] Rakesh Chada, Zhaoheng Zheng, and Pradeep Natarajan. Momo: A shared encoder model for text, image and multi-modal representations. _arXiv preprint arXiv:2304.05523_, 2023.
* [65] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
* [66] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. _arXiv preprint arXiv:1609.08144_, 2016.

* Schneider et al. [2019] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised pre-training for speech recognition. _arXiv preprint arXiv:1904.05862_, 2019.
* Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. _arXiv preprint arXiv:1804.07461_, 2018.
* Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, pages 248-255. Ieee, 2009.
* Wang et al. [2021] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In _ICCV_, 2021.
* Liu et al. [2022] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. _arXiv preprint arXiv:2201.03545_, 2022.
* Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In _ECCV_, 2014.
* He et al. [2017] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In _ICCV_, pages 2961-2969, 2017.
* Xiao et al. [2018] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In _ECCV_, pages 418-434, 2018.
* Zhou et al. [2017] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 633-641, 2017.
* Nguyen et al. [2017] Dat Tien Nguyen, Hyung Gil Hong, Ki Wan Kim, and Kang Ryoung Park. Person recognition system based on a combination of body images from visible light and thermal cameras. _Sensors_, 17(3):605, 2017.
* Rahman et al. [2020] Tawsifur Rahman, Amith Khandakar, Muhammad Abdul Kadir, Khandaker Rejaul Islam, Khandakar F Islam, Rashid Mazhar, Tahir Hamid, Mohammad Tariqul Islam, Saad Kashem, Zaid Bin Mahbub, et al. Reliable tuberculosis detection using chest x-ray with deep learning, segmentation and visualization. _IEEE Access_, 8:191586-191601, 2020.
* Wu et al. [2015] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In _CVPR_, 2015.
* Armeni et al. [2016] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces. In _CVPR_, pages 1534-1543, 2016.
* Yi et al. [2016] Li Yi, Vladimir G Kim, Duygu Ceylan, I Shen, Mengyan Yan, Hao Su, ARCewu Lu, Qixing Huang, Alla Sheffer, Leonidas Guibas, et al. A scalable active framework for region annotation in 3d shape collections. _ACM TOG_, 35(6):210, 2016.
* Warden [2018] Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. _arXiv preprint arXiv:1804.03209_, 2018.
* Soomro et al. [2012] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. _arXiv preprint arXiv:1212.0402_, 2012.
* Zhou et al. [2021] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _AAAI_, 2021.
* Lai et al. [2018] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In _The 41st international ACM SIGIR conference on research & development in information retrieval_, pages 95-104, 2018.

* [85] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with Auto-Correlation for long-term series forecasting. In _NeurIPS_, 2021.
* [86] Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A large-scale challenge for machine learning on graphs. _arXiv preprint arXiv:2103.09430_, 2021.
* [87] Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. Tabtransformer: Tabular data modeling using contextual embeddings. _arXiv preprint arXiv:2012.06678_, 2020.
* [88] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18995-19012, 2022.
* [89] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
* [90] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. _Advances in Neural Information Processing Systems_, 34:3965-3977, 2021.
* [91] Hugo Touvron, Matthieu Cord, and Herve Jegou. Deit iii: Revenge of the vit. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXIV_, pages 516-533. Springer, 2022.
* [92] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12009-12019, 2022.
* [93] Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong Han, Guiguang Ding, and Jian Sun. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In _CVPR_, 2022.
* [94] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser Nam Lim, and Jiwen Lu. Hornet: Efficient high-order spatial interactions with recursive gated convolutions. _Advances in Neural Information Processing Systems_, 35:10353-10366, 2022.
* [95] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _CVPR_, 2022.
* [96] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Interimimage: Exploring large-scale vision foundation models with deformable convolutions. _arXiv preprint arXiv:2211.05778_, 2022.
* [97] Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, and Steven C. H. Hoi. Deep learning for person re-identification: A survey and outlook. _arXiv preprint arXiv:2001.04193_, 2020.
* [98] Ziyu Wei, Xi Yang, Nannan Wang, and Xinbo Gao. Syncrete modality collaborative learning for visible infrared person re-identification. In _ICCV_, pages 225-234, October 2021.
* [99] Yiyuan Zhang, Sanyuan Zhao, Yuhao Kang, and Jianbing Shen. Modality synergy complement learning with cascaded aggregation for visible-infrared person re-identification. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XIV_, pages 462-479. Springer, 2022.
* [100] Danfeng Hong, Zhu Han, Jing Yao, Lianru Gao, Bing Zhang, Antonio Plaza, and Jocelyn Chanussot. Spectralformer: Rethinking hyperspectral image classification with transformers. _IEEE Transactions on Geoscience and Remote Sensing_, 60:1-15, 2021.
* [101] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. _TOG_, 2019.

* [102] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [103] Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. Rethinking network design and local geometry in point cloud: A simple residual mlp framework. _ICLR_, 2022.
* [104] Jaesung Choe, Chunghyun Park, Francois Rameau, Jaesik Park, and In So Kweon. Pointmixer: Mlp-mixer for point cloud understanding. In _European Conference on Computer Vision_, pages 620-640. Springer, 2022.
* [105] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, and Li Yuan. Masked autoencoders for point cloud self-supervised learning. _arXiv preprint arXiv:2203.06604_, 2022.
* [106] Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, and Kaisheng Ma. Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning? _arXiv preprint arXiv:2212.08320_, 2022.
* [107] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In _CVPR_, 2022.
* [108] Faris Almalik, Mohammad Yaqub, and Karthik Nandakumar. Self-ensembling vision transformer (sevit) for robust medical image classification. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference, Singapore, September 18-22, 2022, Proceedings, Part III_, pages 376-386. Springer, 2022.
* [109] Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Unsupervised representation learning by sorting sequence. In _ICCV_, 2017.
* [110] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. In _CVPR_, 2021.
* [111] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. _arXiv preprint arXiv:2203.12602_, 2022.
* [112] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. _arXiv preprint arXiv:2303.16727_, 2023.
* [113] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. _arXiv preprint arXiv:2210.02186_, 2022.
* [114] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H. Hoi. Etsformer: Exponential smoothing transformers for time-series forecasting. _arXiv preprint arXiv:2202.01381_, 2022.
* [115] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In _ICML_, 2022.
* [116] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Rethinking the stationarity in time series forecasting. In _NeurIPS_, 2022.
* [117] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In _ICLR_, 2021.
* [118] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In _NeurIPS_, 2019.
* [119] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In _ICLR_, 2020.

* [120] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _ICLR_. OpenReview.net, 2017.
* [121] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations_, 2019.
* [122] Remy Brossard, Oriel Frigo, and David Dehaene. Graph convolutions that can finally model local structure. _arXiv preprint arXiv:2011.15069_, 2020.
* [123] Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train deeper gcns. _arXiv preprint arXiv:2006.07739_, 2020.
* [124] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. _AAAI Workshop on Deep Learning on Graphs: Methods and Applications_, 2021.
* [125] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? In _Thirty-Fifth Conference on Neural Information Processing Systems_, 2021.
* [126] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, and Yiran Zhong. Audio-visual segmentation. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVII_, pages 386-403. Springer, 2022.

## Appendix A Summary

The appendix is organized as the following:

* We first validate and discuss the potential of the Meta-Transformer on more modalities (video, infrared, X-Ray, and hyperspectral images) in addition to the modalities shown in the main paper, and we provide surprising experimental results on these modalities in SS B.
* Then we further demonstrate the performance and merits of Meta-Transformer in dealing with multi-modal tasks (involving inputs from more than one modality to perform predictions) in SS C.
* In addition, we introduce more details of experiments on text, image, point cloud, and audio in SS D.
* Last but not least, we discuss the impact of Meta-Transformer on the machine learning and computer vision community in SS E.

## Appendix B Extensibility on Single-Modality Perception

In the main body of this paper, we illustrate that Meta-Transformer can simultaneously uncover the underlying patterns of natural language, 2D images, 3D point clouds, and audio spectrograms with the same network architecture and network parameters. Furthermore, we explore its ability in perceiving other modalities, like video recognition, infrared, X-Ray, and hyperspectral image recognition. In specific, we conduct experiments on UCF101 [82] (**video**), RegDB [76] (**infrared** images), Chest **X-Ray**[77], and Indian Pine (**hyperspectral** images) datasets.

### Video Recognition

For video recognition, we follow VideoMAE [111] to modify the tokenizer by replacing the 2D embedding layer with a 3D embedding layer to simultaneously encode the spatial-temporal information from input frames. After tokenization, by leveraging the modality-shared encoder and task-specific heads, Meta-Transformer is able to extract high-level semantic features from videos and achieve favorable performance in the action recognition task of the UCF101 dataset.

**Dataset**. The UCF101 [82] dataset is a common-used benchmark dataset for action recognition tasks. It is an extended version of UCF50 and contains 13,320 video clips of 101 categories. These 101 categories can be divided into 5 groups: Body motion, Human-human interactions, Human-object interactions, Playing musical instruments and Sports. All the input frames are with a resolution of 320\(\times\)240 and a fixed frame rate of 25 FPS, collected from YouTube.

### Infrared Image Recognition

Infrared and hyperspectral image recognition poses unique challenges due to their specific characteristics. For infrared images, the Meta-Transformer framework could be adapted to capture thermal information by encoding temperature values alongside visual features, where the tokenizer for infrared images is the same as common RGB images.

**Dataset**. The RegDB [76] dataset focuses on evaluating the performance of infrared recognition algorithms in unconstrained and realistic scenarios. It includes variations in pose, expression, illumination, and occlusion. We conduct experiments on the RegDB dataset to evaluate the performance of Meta-Transformer on infrared recognition.

### Hyperspectral Image Recognition

Similarly, for hyperspectral images, we expect that Meta-Transformer can also handle the high-dimensional spectral information by representing each spectral band in token embeddings. Comparedwith dealing with RGB images, the only modification is that we employ the new linear projection layer to replace the existing 2D convolution layer.

**Dataset**. The Indian Pine dataset is widely used in remote sensing and hyperspectral image analysis. It consists of \(145\times 145\) pixels with 145 spectral bands, which are captured in Indiana.

### X-Ray Image Recognition

In addition, we explore the potential of the Meta-Transformer in medical image analysis. We leverage the tokenizer for RGB images here to encode raw medical images. Specifically, we conduct experiments regarding X-ray image analysis on the Chest X-Ray [77] dataset. It is a collection of medical images commonly used for the analysis and diagnosis of various thoracic conditions. It comprises 7,000 X-ray images of the chest. The dataset is annotated with labels indicating the presence or absence of abnormalities such as lung diseases, fractures, and heart conditions.

## Appendix C Extensibility on Multi-Modality Perception

Since the modalities of text, image, point cloud, and audio are all involved in this paper, we did not conduct comprehensive multi-modal experiments as common practice such as Flamingo [25], OFA [10], or BEiT-3 [3]. Instead, we conduct multi-modal experiments on a new and challenging task of Audio-Visual Segmentation [126], which is mainly focused on building an intelligent listener to align with fundamental visual tasks.

### Audio-Visual Segmentation

Audio-visual segmentation [126] refers to the task of segmenting objects from different audio sources within a referring image. It aims to develop algorithms that analyze both audio and visual signals simultaneously to identify and delineate distinct sources or events. It finds applications in fields like video conferencing, surveillance, multimedia analysis, and augmented reality.

We conduct experiments on the AVSS [126] dataset, which is recently released in the field of audio-visual research. It provides a comprehensive collection of audio and visual data captured in real-world scenarios. The dataset includes synchronized audio and visual recordings, featuring various events of human actions and natural sounds. In contrast to introducing multi-modal fusion modules as existing methods, Meta-Transformer directly concatenates visual and audio embeddings after Data-to-Sequence tokenization. After extracting representation, we employ a simple global average pooling layer to obtain the final representations of two modalities. Table 13 illustrates the performance of

Meta-Transformer and existing methods on the AVSS dataset for audio-visual segmentation. The evaluation metrics reported in this task are mlou and F-score. In comparison, Meta-Transformer outperforms all other methods with the highest mlou of 31.33% and the highest F-score of 0.387. It also stands out for its significantly lower parameter count, with only 86.5 million parameters compared to the approximate 80M to 180M parameters of other methods.

Meta-Transformer offers several advantages over other methods in the field.

* **Unified architecture**. It relieves modality-specific encoders and reduces computation by leveraging a unified encode to process both audio and images, resulting in a more efficient and streamlined process.

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline
**Method** & **mlou (\%)** & **F-score** & **Params** \\ \hline AVSS [126] (ResNet-50) & 20.18 & 0.252 & \(\$0\)M \\ AVSS [126] (ASPP) & 28.94 & - & \(\$180\)M \\ AVSS [126] (PVT-v2) & 29.77 & 0.352 & \(\$180\)M \\ \hline Meta-Transformer & **31.33** & **0.387** & **86.5M** \\ \hline \hline \end{tabular}
\end{table}
Table 13: **Audio-Visual Segmentation with Meta-Transformer**. We conduct experiments on the AVSS [126] dataset, we report mlou (%) and F-score.

* **Faster convergence**. Thanks to the unified architecture for processing both audio and images, the encoder can deeply align the two modalities instead of only at the output end, which leads to faster convergence. Meta-Transformer only needs 4 training epochs to reach 31.33% of mlou.
* **Superior performance**. Meta-Transformer achieves a significant improvement of \(10\%\) compared to other methods of a similar parameter scale.
* **Efficiency**. Despite its enhanced performance, Meta-Transformer achieves this with much fewer parameters, requiring only \(1/3\) of the parameter amount, which makes forward and backward progress ease.

In summary, the benefits of employing the Meta-Transformer to deal with multi-modal tasks are appealing due to computational efficiency, rapid convergence, improved performance, and parameter efficiency. It reveals the significantly promising direction to apply Meta-Transformer to more multi-modal tasks.

## Appendix D Experimental Details

Our code is built on open-source projects including MMClassification8, MMDetection9, MMsegmentation10, OpenPoints11, Time-Series-Library12, Graphomer 13.

Footnote 8: [https://github.com/open-mmlab/mmpretrain/tree/mmcls-1.x](https://github.com/open-mmlab/mmpretrain/tree/mmcls-1.x)

Footnote 9: [https://github.com/open-mmlab/mmedetection](https://github.com/open-mmlab/mmedetection)

Footnote 10: [https://github.com/open-mmlab/mmsegmentation](https://github.com/open-mmlab/mmsegmentation)

Footnote 11: [https://github.com/guochengqian/openpoints](https://github.com/guochengqian/openpoints)

Footnote 12: [https://github.com/thuml/Time-Series-Library](https://github.com/thuml/Time-Series-Library)

Footnote 13: [https://github.com/microsoft/Graphormer](https://github.com/microsoft/Graphormer)

## Appendix E Further Impact Discussion

### Modality-Free Perception

We hope that Meta-Transformer can introduce new insight into both multi-modal learning and multi-modal generation fields. Meta-Transformer enables the usage of a shared encoder to encode diverse modalities, e.g. natural language, 2D images, 3D point clouds, as well as audio spectrograms., and project them into a shared representation space. This naturally reduces the modality gap across modalities and mitigates the burden of cross-modal alignment. In addition, Meta-Transformer removes the need for paired training data (such as image-text pairs), thus endowing multi-modal learning with more training flexibility.

### Application Prospects

We investigate the application of Meta-Transformer on a wide range of modalities including RGB images, text, point clouds, video understanding, remote sensing (hyper-spectral images), nighttime surveillance (infrared images), and medical analysis (X-Ray images).

**In video understanding, Meta-Transformer reveals the potential of enhancing the analysis and interpretation of videos by integrating information from text, audio, and image with the shared encoder. This benefits tasks such as action recognition, event detection, and video summarization. Meta-Transformer's capability to handle video-related modalities paves the way for improved video understanding applications in areas like video surveillance, video indexing, and content-based video retrieval.

**In hyperspectral imaging for remote sensing**, Meta-Transformer enables the analysis and understanding of hyperspectral data by extracting high-level semantic features. It enhances tasks such as classification, target detection, and land cover mapping, improving the accuracy and efficiency of remote sensing applications. The ability to process hyperspectral images using Meta-Transformer opens doors for advancements in environmental monitoring, agriculture, urban planning, and disaster management.

**In medical applications**, particularly X-ray image analysis, Meta-Transformer offers a promising approach to improving diagnostic accuracy and efficiency with multi-modal information. It can effectively capture and fuse information from X-ray images, clinical data, and other modalities to aid in disease detection, anomaly identification, and treatment planning by leveraging its unified learning framework. Meta-Transformer's capability to handle multi-modal data enhances the potential for more accurate and comprehensive medical imaging analysis, leading to better patient care and outcomes.

**For infrared images used in nighttime recognition and surveillance**, Meta-Transformer's ability to process infrared data helps extract crucial information for object detection, tracking, and recognition in low-light conditions, which opens an avenue for advancements in nighttime surveillance, security systems, and autonomous navigation in challenging environments with the cooperation between infrared cameras with RGB cameras.

### Conclusion

In summary, we think that the ability of Meta-Transformer to unify multi-modal learning comes from that _neural network architectures can learn modality-invariant patterns_. The architecture of Meta-Transformer illustrates the advantages of length-variable token embeddings in multi-modal learning, which provides flexible but unified forms of multi-modal semantics. Then it's time to think about designing algorithms to train networks that generalize on _unseen_ modalities. Meanwhile, it's also intriguing to design the architecture of a unified multi-modal decoder, which can decode representations into any form of a specific modality.

Although Meta-Transformer presents a surprising performance and shows a new promising direction in multi-modal perception, we are not sure whether the proposed architectures are also effective in generative tasks. And it remains mysterious how to develop modality-invariant generative models. We hope that this can inspire future research.
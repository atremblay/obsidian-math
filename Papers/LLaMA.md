---
title: "LLaMA: Open and Efficient Foundation Language Models"
url: 
    - "[paper](https://arxiv.org/pdf/2302.13971.pdf)"
    - "[arxiv](https://arxiv.org/abs/2302.13971)"
tags:
    - model
    - llm
published: "2023-02-27"
authors:
    - Hugo Touvron
    - Thibaut Lavril
    - Gautier Izacard
    - Xavier Martinet 
    - "Marie-Anne Lachaux"
    - Timothee Lacroix
    - Baptiste Rozière
    - Naman Goyal Eric Hambro
    - Faisal Azhar
    - Aurelien Rodriguez
    - Armand Joulin
    - Edouard Grave
    - Guillaume Lample
status: "to read"
---

# LLaMA: Open and Efficient Foundation Language Models
###### Authors
<ul>
<li class="author">Hugo Touvron</li>
<li class="separator author">|</li>
<li class="author">Thibaut Lavril</li>
<li class="separator author">|</li>
<li class="author">Gautier Izacard</li>
<li class="separator author">|</li>
<li class="author">Xavier Martinet</li>
<li class="separator author">|</li>
<li class="author">Marie-Anne Lachaux</li>
<li class="separator author">|</li>
<li class="author">Timothee Lacroix</li>
<li class="separator author">|</li>
<li class="author">Baptiste Rozière</li>
<li class="separator author">|</li>
<li class="author">Naman Goyal Eric Hambro</li>
<li class="separator author">|</li>
<li class="author">Faisal Azhar</li>
<li class="separator author">|</li>
<li class="author">Aurelien Rodriguez</li>
<li class="separator author">|</li>
<li class="author">Armand Joulin</li>
<li class="separator author">|</li>
<li class="author">Edouard Grave</li>
<li class="separator author">|</li>
<li class="author">Guillaume Lample</li>
</ul>

```dataview
table without ID url from "Mathematics/Machine Learning/Papers/LLaMA.md"
```

#### Abstract
We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA- 65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.


